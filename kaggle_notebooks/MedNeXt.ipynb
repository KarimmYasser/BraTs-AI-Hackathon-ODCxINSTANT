{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedNeXt Training and Data Preparation\n",
    "This notebook prepares BraTS data and sets up MedNeXt training workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:18:32.096216Z",
     "iopub.status.busy": "2026-02-02T11:18:32.095968Z",
     "iopub.status.idle": "2026-02-02T11:19:52.660382Z",
     "shell.execute_reply": "2026-02-02T11:19:52.659663Z",
     "shell.execute_reply.started": "2026-02-02T11:18:32.096193Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 tar files. Extracting to /kaggle/working/brats_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted: BraTS2021_00495.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/149350238.py:21: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=extract_dir)\n",
      "100%|██████████| 3/3 [01:20<00:00, 26.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted: BraTS2021_Training_Data.tar\n",
      "✅ Extracted: BraTS2021_00621.tar\n",
      "\n",
      "Extraction Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_dir = '/kaggle/input/brats-2021-task1'\n",
    "extract_dir = '/kaggle/working/brats_data'\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "tar_files = glob.glob(os.path.join(input_dir, '*.tar'))\n",
    "\n",
    "print(f\"Found {len(tar_files)} tar files. Extracting to {extract_dir}...\")\n",
    "\n",
    "for tar_path in tqdm(tar_files):\n",
    "    try:\n",
    "        with tarfile.open(tar_path) as tar:\n",
    "            tar.extractall(path=extract_dir)\n",
    "            print(f\"Extracted: {os.path.basename(tar_path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {tar_path}: {e}\")\n",
    "\n",
    "print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:19:52.661596Z",
     "iopub.status.busy": "2026-02-02T11:19:52.661356Z",
     "iopub.status.idle": "2026-02-02T11:19:52.675606Z",
     "shell.execute_reply": "2026-02-02T11:19:52.675039Z",
     "shell.execute_reply.started": "2026-02-02T11:19:52.661574Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root contents: ['BraTS2021_01524', 'BraTS2021_01509', 'BraTS2021_01139', 'BraTS2021_01001', 'BraTS2021_00376']\n",
      "------------------------------\n",
      "YOUR DATA PATH IS: /kaggle/working/brats_data\n",
      "------------------------------\n",
      "Found 1251 subject folders.\n",
      "Sample subjects: ['BraTS2021_01524', 'BraTS2021_01509', 'BraTS2021_01139']\n"
     ]
    }
   ],
   "source": [
    "# List the contents of the extraction folder\n",
    "contents = os.listdir(extract_dir)\n",
    "print(f\"Root contents: {contents[:5]}\") # Print first 5 items\n",
    "\n",
    "# Check if there is a subfolder (common in tar files)\n",
    "if 'BraTS2021_Training_Data' in contents:\n",
    "    final_data_path = os.path.join(extract_dir, 'BraTS2021_Training_Data')\n",
    "else:\n",
    "    final_data_path = extract_dir\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"YOUR DATA PATH IS: {final_data_path}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Verify we can see subject folders (e.g., BraTS2021_00001)\n",
    "subjects = [x for x in os.listdir(final_data_path) if os.path.isdir(os.path.join(final_data_path, x))]\n",
    "print(f\"Found {len(subjects)} subject folders.\")\n",
    "print(f\"Sample subjects: {subjects[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:19:52.677693Z",
     "iopub.status.busy": "2026-02-02T11:19:52.677407Z",
     "iopub.status.idle": "2026-02-02T11:20:35.007750Z",
     "shell.execute_reply": "2026-02-02T11:20:35.006729Z",
     "shell.execute_reply.started": "2026-02-02T11:19:52.677663Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'mednext' already exists and is not an empty directory.\n",
      "Obtaining file:///kaggle/working/mednext\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch>1.10.0 in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (2.8.0+cu126)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (4.67.1)\n",
      "Requirement already satisfied: dicom2nifti in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (2.6.2)\n",
      "Requirement already satisfied: scikit-image>=0.14 in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (0.25.2)\n",
      "Requirement already satisfied: medpy in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (0.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (1.15.3)\n",
      "Requirement already satisfied: batchgenerators>=0.23 in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (0.25.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (1.6.1)\n",
      "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (2.5.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (2.2.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (2.32.5)\n",
      "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (5.3.2)\n",
      "Requirement already satisfied: tifffile in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (2025.10.16)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (3.10.0)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from batchgenerators>=0.23->mednextv1==1.7.0) (11.3.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from batchgenerators>=0.23->mednextv1==1.7.0) (1.0.0)\n",
      "Requirement already satisfied: unittest2 in /usr/local/lib/python3.12/dist-packages (from batchgenerators>=0.23->mednextv1==1.7.0) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.12/dist-packages (from batchgenerators>=0.23->mednextv1==1.7.0) (3.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (3.5)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (2.37.0)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (26.0rc2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (3.4.0)\n",
      "Requirement already satisfied: pydicom>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dicom2nifti->mednextv1==1.7.0) (3.0.1)\n",
      "Requirement already satisfied: python-gdcm in /usr/local/lib/python3.12/dist-packages (from dicom2nifti->mednextv1==1.7.0) (3.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mednextv1==1.7.0) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mednextv1==1.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mednextv1==1.7.0) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mednextv1==1.7.0) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mednextv1==1.7.0) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mednextv1==1.7.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->mednextv1==1.7.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->mednextv1==1.7.0) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->mednextv1==1.7.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->mednextv1==1.7.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->mednextv1==1.7.0) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->mednextv1==1.7.0) (2026.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->mednextv1==1.7.0) (1.5.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mednextv1==1.7.0) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>1.10.0->mednextv1==1.7.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>1.10.0->mednextv1==1.7.0) (3.0.3)\n",
      "Collecting argparse (from unittest2->batchgenerators>=0.23->mednextv1==1.7.0)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: traceback2 in /usr/local/lib/python3.12/dist-packages (from unittest2->batchgenerators>=0.23->mednextv1==1.7.0) (1.4.0)\n",
      "Requirement already satisfied: linecache2 in /usr/local/lib/python3.12/dist-packages (from traceback2->unittest2->batchgenerators>=0.23->mednextv1==1.7.0) (1.0.0)\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: argparse, mednextv1\n",
      "  Attempting uninstall: mednextv1\n",
      "    Found existing installation: mednextv1 1.7.0\n",
      "    Uninstalling mednextv1-1.7.0:\n",
      "      Successfully uninstalled mednextv1-1.7.0\n",
      "  Running setup.py develop for mednextv1\n",
      "Successfully installed argparse-1.4.0 mednextv1-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/MIC-DKFZ/MedNeXt.git mednext\n",
    "!pip install -e ./mednext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:20:35.009381Z",
     "iopub.status.busy": "2026-02-02T11:20:35.009093Z",
     "iopub.status.idle": "2026-02-02T11:20:38.366133Z",
     "shell.execute_reply": "2026-02-02T11:20:38.365367Z",
     "shell.execute_reply.started": "2026-02-02T11:20:35.009350Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (5.3.2)\n",
      "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.12/dist-packages (2.5.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: torchio in /usr/local/lib/python3.12/dist-packages (0.21.2)\n",
      "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from nibabel) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from nibabel) (26.0rc2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.12/dist-packages (from nibabel) (4.15.0)\n",
      "Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.12/dist-packages (from torchio) (1.3.1)\n",
      "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.12/dist-packages (from torchio) (0.8.1)\n",
      "Requirement already satisfied: humanize>=0.1 in /usr/local/lib/python3.12/dist-packages (from torchio) (4.14.0)\n",
      "Requirement already satisfied: rich>=10 in /usr/local/lib/python3.12/dist-packages (from torchio) (14.2.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.12/dist-packages (from torchio) (1.15.3)\n",
      "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.12/dist-packages (from torchio) (2.8.0+cu126)\n",
      "Requirement already satisfied: typer>=0.1 in /usr/local/lib/python3.12/dist-packages (from torchio) (0.20.0)\n",
      "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated>=1.2->torchio) (2.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10->torchio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10->torchio) (2.19.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (3.4.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.1->torchio) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.1->torchio) (1.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10->torchio) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.9->torchio) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.9->torchio) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nibabel SimpleITK tqdm torchio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:20:38.367593Z",
     "iopub.status.busy": "2026-02-02T11:20:38.367329Z",
     "iopub.status.idle": "2026-02-02T11:20:38.374705Z",
     "shell.execute_reply": "2026-02-02T11:20:38.373957Z",
     "shell.execute_reply.started": "2026-02-02T11:20:38.367564Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0+cu126\n",
      "TorchIO Version: 0.21.2\n",
      "CUDA Available: True\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "GPU Memory: 17.06 GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'mednext')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# TorchIO for advanced 3D medical image augmentations\n",
    "import torchio as tio\n",
    "\n",
    "# Import MedNeXt\n",
    "from nnunet_mednext import create_mednext_v1, MedNeXt\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"TorchIO Version: {tio.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:20:38.398049Z",
     "iopub.status.busy": "2026-02-02T11:20:38.397661Z",
     "iopub.status.idle": "2026-02-02T11:20:38.422287Z",
     "shell.execute_reply": "2026-02-02T11:20:38.421654Z",
     "shell.execute_reply.started": "2026-02-02T11:20:38.398018Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BraTS2021Dataset class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class BraTS2021Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    BraTS2021 Dataset for brain tumor segmentation.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - 4 MRI modalities: T1, T1ce, T2, FLAIR\n",
    "    - Segmentation mask with labels: 0 (background), 1 (NCR/NET), 2 (ED), 4 (ET)\n",
    "    \n",
    "    Uses patch-based sampling for memory efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        data_dir: str, \n",
    "        subject_ids: List[str],\n",
    "        patch_size: Tuple[int, int, int] = (128, 128, 128),\n",
    "        samples_per_volume: int = 2,\n",
    "        is_training: bool = True,\n",
    "        augment: bool = True\n",
    "    ):\n",
    "        self.data_dir = data_dir\n",
    "        self.subject_ids = subject_ids\n",
    "        self.patch_size = patch_size\n",
    "        self.samples_per_volume = samples_per_volume\n",
    "        self.is_training = is_training\n",
    "        self.augment = augment and is_training\n",
    "        \n",
    "        # Modality suffixes in BraTS2021\n",
    "        self.modalities = ['t1', 't1ce', 't2', 'flair']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subject_ids) * self.samples_per_volume\n",
    "    \n",
    "    def _load_nifti(self, filepath: str) -> np.ndarray:\n",
    "        \"\"\"Load a NIfTI file and return the data array.\"\"\"\n",
    "        img = nib.load(filepath)\n",
    "        return img.get_fdata().astype(np.float32)\n",
    "    \n",
    "    def _normalize(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Z-score normalization per volume (non-zero voxels only).\"\"\"\n",
    "        mask = data > 0\n",
    "        if mask.sum() > 0:\n",
    "            mean = data[mask].mean()\n",
    "            std = data[mask].std()\n",
    "            if std > 0:\n",
    "                data = (data - mean) / std\n",
    "                data[~mask] = 0\n",
    "        return data\n",
    "    \n",
    "    def _convert_labels(self, seg: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert BraTS labels to consecutive integers.\n",
    "        BraTS: 0 (background), 1 (NCR/NET), 2 (ED), 4 (ET)\n",
    "        Output: 0 (background), 1 (NCR/NET), 2 (ED), 3 (ET)\n",
    "        \"\"\"\n",
    "        new_seg = np.zeros_like(seg)\n",
    "        new_seg[seg == 0] = 0\n",
    "        new_seg[seg == 1] = 1\n",
    "        new_seg[seg == 2] = 2\n",
    "        new_seg[seg == 4] = 3\n",
    "        return new_seg\n",
    "    \n",
    "    def _get_random_patch(\n",
    "        self, \n",
    "        volume: np.ndarray, \n",
    "        seg: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Extract a random patch from the volume.\"\"\"\n",
    "        c, d, h, w = volume.shape\n",
    "        pd, ph, pw = self.patch_size\n",
    "        \n",
    "        # Ensure we don't go out of bounds\n",
    "        d_start = random.randint(0, max(0, d - pd))\n",
    "        h_start = random.randint(0, max(0, h - ph))\n",
    "        w_start = random.randint(0, max(0, w - pw))\n",
    "        \n",
    "        # Extract patch\n",
    "        volume_patch = volume[:, d_start:d_start+pd, h_start:h_start+ph, w_start:w_start+pw]\n",
    "        seg_patch = seg[d_start:d_start+pd, h_start:h_start+ph, w_start:w_start+pw]\n",
    "        \n",
    "        # Pad if necessary\n",
    "        if volume_patch.shape[1:] != self.patch_size:\n",
    "            pad_d = pd - volume_patch.shape[1]\n",
    "            pad_h = ph - volume_patch.shape[2]\n",
    "            pad_w = pw - volume_patch.shape[3]\n",
    "            volume_patch = np.pad(volume_patch, ((0, 0), (0, pad_d), (0, pad_h), (0, pad_w)))\n",
    "            seg_patch = np.pad(seg_patch, ((0, pad_d), (0, pad_h), (0, pad_w)))\n",
    "        \n",
    "        return volume_patch, seg_patch\n",
    "    \n",
    "    def _get_tumor_centered_patch(\n",
    "        self, \n",
    "        volume: np.ndarray, \n",
    "        seg: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Extract a patch centered on tumor region for better sampling.\"\"\"\n",
    "        c, d, h, w = volume.shape\n",
    "        pd, ph, pw = self.patch_size\n",
    "        \n",
    "        # Find tumor voxels\n",
    "        tumor_mask = seg > 0\n",
    "        if tumor_mask.sum() > 0:\n",
    "            tumor_coords = np.where(tumor_mask)\n",
    "            # Random tumor voxel as center\n",
    "            idx = random.randint(0, len(tumor_coords[0]) - 1)\n",
    "            center_d = tumor_coords[0][idx]\n",
    "            center_h = tumor_coords[1][idx]\n",
    "            center_w = tumor_coords[2][idx]\n",
    "            \n",
    "            # Calculate start positions\n",
    "            d_start = max(0, min(center_d - pd // 2, d - pd))\n",
    "            h_start = max(0, min(center_h - ph // 2, h - ph))\n",
    "            w_start = max(0, min(center_w - pw // 2, w - pw))\n",
    "        else:\n",
    "            # Fallback to random patch\n",
    "            return self._get_random_patch(volume, seg)\n",
    "        \n",
    "        # Extract patch\n",
    "        volume_patch = volume[:, d_start:d_start+pd, h_start:h_start+ph, w_start:w_start+pw]\n",
    "        seg_patch = seg[d_start:d_start+pd, h_start:h_start+ph, w_start:w_start+pw]\n",
    "        \n",
    "        # Pad if necessary\n",
    "        if volume_patch.shape[1:] != self.patch_size:\n",
    "            pad_d = pd - volume_patch.shape[1]\n",
    "            pad_h = ph - volume_patch.shape[2]\n",
    "            pad_w = pw - volume_patch.shape[3]\n",
    "            volume_patch = np.pad(volume_patch, ((0, 0), (0, pad_d), (0, pad_h), (0, pad_w)))\n",
    "            seg_patch = np.pad(seg_patch, ((0, pad_d), (0, pad_h), (0, pad_w)))\n",
    "        \n",
    "        return volume_patch, seg_patch\n",
    "    \n",
    "    def _augment(self, volume: np.ndarray, seg: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Apply simple augmentations.\"\"\"\n",
    "        # Random flips\n",
    "        if random.random() > 0.5:\n",
    "            volume = np.flip(volume, axis=1).copy()\n",
    "            seg = np.flip(seg, axis=0).copy()\n",
    "        if random.random() > 0.5:\n",
    "            volume = np.flip(volume, axis=2).copy()\n",
    "            seg = np.flip(seg, axis=1).copy()\n",
    "        if random.random() > 0.5:\n",
    "            volume = np.flip(volume, axis=3).copy()\n",
    "            seg = np.flip(seg, axis=2).copy()\n",
    "        \n",
    "        # Random intensity scaling (for each modality independently)\n",
    "        if random.random() > 0.5:\n",
    "            scale = random.uniform(0.9, 1.1)\n",
    "            volume = volume * scale\n",
    "        \n",
    "        return volume, seg\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Determine which subject and which sample from that subject\n",
    "        subject_idx = idx // self.samples_per_volume\n",
    "        subject_id = self.subject_ids[subject_idx]\n",
    "        subject_dir = os.path.join(self.data_dir, subject_id)\n",
    "        \n",
    "        # Load all modalities\n",
    "        modality_data = []\n",
    "        for mod in self.modalities:\n",
    "            filepath = os.path.join(subject_dir, f\"{subject_id}_{mod}.nii.gz\")\n",
    "            data = self._load_nifti(filepath)\n",
    "            data = self._normalize(data)\n",
    "            modality_data.append(data)\n",
    "        \n",
    "        # Stack modalities: (4, D, H, W)\n",
    "        volume = np.stack(modality_data, axis=0)\n",
    "        \n",
    "        # Load segmentation\n",
    "        seg_path = os.path.join(subject_dir, f\"{subject_id}_seg.nii.gz\")\n",
    "        seg = self._load_nifti(seg_path)\n",
    "        seg = self._convert_labels(seg)\n",
    "        \n",
    "        # Extract patch (alternate between random and tumor-centered)\n",
    "        if self.is_training:\n",
    "            if random.random() > 0.5:\n",
    "                volume_patch, seg_patch = self._get_tumor_centered_patch(volume, seg)\n",
    "            else:\n",
    "                volume_patch, seg_patch = self._get_random_patch(volume, seg)\n",
    "        else:\n",
    "            # For validation, use center patch\n",
    "            volume_patch, seg_patch = self._get_tumor_centered_patch(volume, seg)\n",
    "        \n",
    "        # Apply augmentations\n",
    "        if self.augment:\n",
    "            volume_patch, seg_patch = self._augment(volume_patch, seg_patch)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        volume_tensor = torch.from_numpy(volume_patch.copy()).float()\n",
    "        seg_tensor = torch.from_numpy(seg_patch.copy()).long()\n",
    "        \n",
    "        return volume_tensor, seg_tensor\n",
    "\n",
    "print(\"BraTS2021Dataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:32:14.268205Z",
     "iopub.status.busy": "2026-02-02T11:32:14.267848Z",
     "iopub.status.idle": "2026-02-02T11:32:14.277940Z",
     "shell.execute_reply": "2026-02-02T11:32:14.277161Z",
     "shell.execute_reply.started": "2026-02-02T11:32:14.268175Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model size: B, Kernel size: 3x3x3\n"
     ]
    }
   ],
   "source": [
    "# ============== Configuration ==============\n",
    "\n",
    "class Config:\n",
    "    # Data paths\n",
    "    DATA_DIR = final_data_path\n",
    "    CHECKPOINT_DIR = \"checkpoints\"\n",
    "    \n",
    "    # Model configuration\n",
    "    MODEL_SIZE = 'B'          # 'S' (Small), 'B' (Base), 'M' (Medium), 'L' (Large)\n",
    "    KERNEL_SIZE = 3           # 3 or 5\n",
    "    IN_CHANNELS = 4           # 4 MRI modalities (T1, T1ce, T2, FLAIR)\n",
    "    NUM_CLASSES = 4           # Background + 3 tumor regions\n",
    "    DEEP_SUPERVISION = True   # Use deep supervision for training\n",
    "    ACCUMULATION_STEPS = 1    # Gradient accumulation steps\n",
    "    \n",
    "    # Training configuration\n",
    "    BATCH_SIZE = 1            # Due to 3D volumes, usually batch size 1-2\n",
    "    NUM_EPOCHS = 20\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    \n",
    "    # Patch-based training (for memory efficiency)\n",
    "    PATCH_SIZE = (128, 128, 128)  # Size of patches to extract from volumes\n",
    "    SAMPLES_PER_VOLUME = 2        # Number of patches per volume per epoch\n",
    "    \n",
    "    # Data split\n",
    "    TRAIN_RATIO = 0.8\n",
    "    VAL_RATIO = 0.2\n",
    "    \n",
    "    # Mixed precision training\n",
    "    USE_AMP = True\n",
    "    \n",
    "    # Random seed for reproducibility\n",
    "    SEED = 42\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(Config.SEED)\n",
    "np.random.seed(Config.SEED)\n",
    "random.seed(Config.SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(Config.SEED)\n",
    "\n",
    "print(f\"Using device: {Config.DEVICE}\")\n",
    "print(f\"Model size: {Config.MODEL_SIZE}, Kernel size: {Config.KERNEL_SIZE}x{Config.KERNEL_SIZE}x{Config.KERNEL_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:20:38.423640Z",
     "iopub.status.busy": "2026-02-02T11:20:38.423343Z",
     "iopub.status.idle": "2026-02-02T11:20:38.453882Z",
     "shell.execute_reply": "2026-02-02T11:20:38.453138Z",
     "shell.execute_reply.started": "2026-02-02T11:20:38.423605Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subjects found: 1251\n",
      "Training subjects: 1000\n",
      "Validation subjects: 251\n",
      "\n",
      "First few training subjects: ['BraTS2021_01417', 'BraTS2021_01632', 'BraTS2021_01664', 'BraTS2021_01305', 'BraTS2021_01339']\n",
      "First few validation subjects: ['BraTS2021_00570', 'BraTS2021_00003', 'BraTS2021_00555', 'BraTS2021_00238', 'BraTS2021_00094']\n"
     ]
    }
   ],
   "source": [
    "# Get all subject IDs from the data directory\n",
    "all_subjects = sorted([\n",
    "    d for d in os.listdir(Config.DATA_DIR) \n",
    "    if os.path.isdir(os.path.join(Config.DATA_DIR, d)) and d.startswith('BraTS2021_')\n",
    "])\n",
    "\n",
    "print(f\"Total subjects found: {len(all_subjects)}\")\n",
    "\n",
    "# Split into train and validation\n",
    "random.shuffle(all_subjects)\n",
    "split_idx = int(len(all_subjects) * Config.TRAIN_RATIO)\n",
    "train_subjects = all_subjects[:split_idx]\n",
    "val_subjects = all_subjects[split_idx:]\n",
    "\n",
    "print(f\"Training subjects: {len(train_subjects)}\")\n",
    "print(f\"Validation subjects: {len(val_subjects)}\")\n",
    "print(f\"\\nFirst few training subjects: {train_subjects[:5]}\")\n",
    "print(f\"First few validation subjects: {val_subjects[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:20:38.456634Z",
     "iopub.status.busy": "2026-02-02T11:20:38.456347Z",
     "iopub.status.idle": "2026-02-02T11:20:38.463435Z",
     "shell.execute_reply": "2026-02-02T11:20:38.462688Z",
     "shell.execute_reply.started": "2026-02-02T11:20:38.456600Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples per epoch: 2000\n",
      "Validation samples per epoch: 251\n",
      "Training batches per epoch: 2000\n",
      "Validation batches per epoch: 251\n"
     ]
    }
   ],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = BraTS2021Dataset(\n",
    "    data_dir=Config.DATA_DIR,\n",
    "    subject_ids=train_subjects,\n",
    "    patch_size=Config.PATCH_SIZE,\n",
    "    samples_per_volume=Config.SAMPLES_PER_VOLUME,\n",
    "    is_training=True,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "val_dataset = BraTS2021Dataset(\n",
    "    data_dir=Config.DATA_DIR,\n",
    "    subject_ids=val_subjects,\n",
    "    patch_size=Config.PATCH_SIZE,\n",
    "    samples_per_volume=1,  # One sample per volume for validation\n",
    "    is_training=False,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Training samples per epoch: {len(train_dataset)}\")\n",
    "print(f\"Validation samples per epoch: {len(val_dataset)}\")\n",
    "print(f\"Training batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Validation batches per epoch: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:20:38.485262Z",
     "iopub.status.busy": "2026-02-02T11:20:38.485007Z",
     "iopub.status.idle": "2026-02-02T11:20:45.229502Z",
     "shell.execute_reply": "2026-02-02T11:20:45.228682Z",
     "shell.execute_reply.started": "2026-02-02T11:20:38.485240Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume shape: torch.Size([1, 4, 128, 128, 128])\n",
      "Segmentation shape: torch.Size([1, 128, 128, 128])\n",
      "Volume dtype: torch.float32\n",
      "Segmentation dtype: torch.int64\n",
      "Unique segmentation values: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# Verify a sample batch\n",
    "sample_volume, sample_seg = next(iter(train_loader))\n",
    "print(f\"Volume shape: {sample_volume.shape}\")  # Expected: (B, 4, 128, 128, 128)\n",
    "print(f\"Segmentation shape: {sample_seg.shape}\")  # Expected: (B, 128, 128, 128)\n",
    "print(f\"Volume dtype: {sample_volume.dtype}\")\n",
    "print(f\"Segmentation dtype: {sample_seg.dtype}\")\n",
    "print(f\"Unique segmentation values: {torch.unique(sample_seg).tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:20:45.231492Z",
     "iopub.status.busy": "2026-02-02T11:20:45.230993Z",
     "iopub.status.idle": "2026-02-02T11:20:45.245688Z",
     "shell.execute_reply": "2026-02-02T11:20:45.244912Z",
     "shell.execute_reply.started": "2026-02-02T11:20:45.231462Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Soft Dice Loss for multi-class segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth: float = 1e-5, include_background: bool = False):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        self.include_background = include_background\n",
    "    \n",
    "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: (B, C, D, H, W) - softmax probabilities\n",
    "            targets: (B, D, H, W) - integer class labels\n",
    "        \"\"\"\n",
    "        num_classes = predictions.shape[1]\n",
    "        \n",
    "        # Convert targets to one-hot\n",
    "        targets_one_hot = F.one_hot(targets, num_classes)  # (B, D, H, W, C)\n",
    "        targets_one_hot = targets_one_hot.permute(0, 4, 1, 2, 3).float()  # (B, C, D, H, W)\n",
    "        \n",
    "        # Skip background class if specified\n",
    "        start_idx = 0 if self.include_background else 1\n",
    "        \n",
    "        dice_scores = []\n",
    "        for c in range(start_idx, num_classes):\n",
    "            pred_c = predictions[:, c]\n",
    "            target_c = targets_one_hot[:, c]\n",
    "            \n",
    "            intersection = (pred_c * target_c).sum()\n",
    "            union = pred_c.sum() + target_c.sum()\n",
    "            \n",
    "            dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "            dice_scores.append(dice)\n",
    "        \n",
    "        # Average Dice across classes\n",
    "        mean_dice = torch.stack(dice_scores).mean()\n",
    "        return 1.0 - mean_dice\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Dice Loss + Cross-Entropy Loss for segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        dice_weight: float = 1.0, \n",
    "        ce_weight: float = 1.0,\n",
    "        include_background: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_weight = ce_weight\n",
    "        self.dice_loss = DiceLoss(include_background=include_background)\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: (B, C, D, H, W) - logits\n",
    "            targets: (B, D, H, W) - integer class labels\n",
    "        \"\"\"\n",
    "        # Softmax for Dice loss\n",
    "        probs = F.softmax(predictions, dim=1)\n",
    "        dice_loss = self.dice_loss(probs, targets)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        ce_loss = self.ce_loss(predictions, targets)\n",
    "        \n",
    "        return self.dice_weight * dice_loss + self.ce_weight * ce_loss\n",
    "\n",
    "\n",
    "class DeepSupervisionLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function for deep supervision with multiple output scales.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_loss: nn.Module, weights: List[float] = None):\n",
    "        super().__init__()\n",
    "        self.base_loss = base_loss\n",
    "        self.weights = weights\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        predictions: List[torch.Tensor], \n",
    "        targets: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: List of (B, C, D, H, W) tensors at different scales\n",
    "            targets: (B, D, H, W) - integer class labels at full resolution\n",
    "        \"\"\"\n",
    "        if not isinstance(predictions, (list, tuple)):\n",
    "            # Single output (no deep supervision)\n",
    "            return self.base_loss(predictions, targets)\n",
    "        \n",
    "        n_outputs = len(predictions)\n",
    "        if self.weights is None:\n",
    "            # Default: exponentially decreasing weights\n",
    "            self.weights = [1.0 / (2 ** i) for i in range(n_outputs)]\n",
    "            total = sum(self.weights)\n",
    "            self.weights = [w / total for w in self.weights]\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        for i, pred in enumerate(predictions):\n",
    "            # Resize target to match prediction size\n",
    "            if pred.shape[2:] != targets.shape[1:]:\n",
    "                target_resized = F.interpolate(\n",
    "                    targets.unsqueeze(1).float(), \n",
    "                    size=pred.shape[2:], \n",
    "                    mode='nearest'\n",
    "                ).squeeze(1).long()\n",
    "            else:\n",
    "                target_resized = targets\n",
    "            \n",
    "            total_loss += self.weights[i] * self.base_loss(pred, target_resized)\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "\n",
    "print(\"Loss functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:20:45.246996Z",
     "iopub.status.busy": "2026-02-02T11:20:45.246755Z",
     "iopub.status.idle": "2026-02-02T11:20:45.273737Z",
     "shell.execute_reply": "2026-02-02T11:20:45.273065Z",
     "shell.execute_reply.started": "2026-02-02T11:20:45.246974Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def compute_dice_score(\n",
    "    predictions: torch.Tensor, \n",
    "    targets: torch.Tensor, \n",
    "    num_classes: int = 4,\n",
    "    include_background: bool = False\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute Dice score for each class.\n",
    "    \n",
    "    Args:\n",
    "        predictions: (B, C, D, H, W) - softmax probabilities or logits\n",
    "        targets: (B, D, H, W) - integer class labels\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with Dice scores for each class\n",
    "    \"\"\"\n",
    "    if predictions.shape[1] != num_classes:\n",
    "        raise ValueError(f\"Expected {num_classes} classes, got {predictions.shape[1]}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    pred_classes = predictions.argmax(dim=1)  # (B, D, H, W)\n",
    "    \n",
    "    dice_scores = {}\n",
    "    class_names = ['Background', 'NCR/NET', 'Edema', 'ET']\n",
    "    \n",
    "    start_idx = 0 if include_background else 1\n",
    "    \n",
    "    for c in range(start_idx, num_classes):\n",
    "        pred_c = (pred_classes == c).float()\n",
    "        target_c = (targets == c).float()\n",
    "        \n",
    "        intersection = (pred_c * target_c).sum()\n",
    "        union = pred_c.sum() + target_c.sum()\n",
    "        \n",
    "        if union > 0:\n",
    "            dice = (2.0 * intersection) / union\n",
    "        else:\n",
    "            dice = torch.tensor(1.0) if intersection == 0 else torch.tensor(0.0)\n",
    "        \n",
    "        dice_scores[class_names[c]] = dice.item()\n",
    "    \n",
    "    # Compute mean Dice (excluding background)\n",
    "    tumor_dices = [dice_scores[name] for name in class_names[1:]]\n",
    "    dice_scores['Mean'] = np.mean(tumor_dices)\n",
    "    \n",
    "    # BraTS-specific regions\n",
    "    # Whole Tumor (WT): all tumor classes (1, 2, 3)\n",
    "    pred_wt = (pred_classes > 0).float()\n",
    "    target_wt = (targets > 0).float()\n",
    "    wt_inter = (pred_wt * target_wt).sum()\n",
    "    wt_union = pred_wt.sum() + target_wt.sum()\n",
    "    dice_scores['WT'] = (2.0 * wt_inter / (wt_union + 1e-8)).item()\n",
    "    \n",
    "    # Tumor Core (TC): NCR/NET + ET (classes 1 and 3)\n",
    "    pred_tc = ((pred_classes == 1) | (pred_classes == 3)).float()\n",
    "    target_tc = ((targets == 1) | (targets == 3)).float()\n",
    "    tc_inter = (pred_tc * target_tc).sum()\n",
    "    tc_union = pred_tc.sum() + target_tc.sum()\n",
    "    dice_scores['TC'] = (2.0 * tc_inter / (tc_union + 1e-8)).item()\n",
    "    \n",
    "    # Enhancing Tumor (ET): class 3\n",
    "    pred_et = (pred_classes == 3).float()\n",
    "    target_et = (targets == 3).float()\n",
    "    et_inter = (pred_et * target_et).sum()\n",
    "    et_union = pred_et.sum() + target_et.sum()\n",
    "    dice_scores['ET'] = (2.0 * et_inter / (et_union + 1e-8)).item()\n",
    "    \n",
    "    return dice_scores\n",
    "\n",
    "print(\"Evaluation metrics defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:20:45.274919Z",
     "iopub.status.busy": "2026-02-02T11:20:45.274665Z",
     "iopub.status.idle": "2026-02-02T11:20:45.439711Z",
     "shell.execute_reply": "2026-02-02T11:20:45.439031Z",
     "shell.execute_reply.started": "2026-02-02T11:20:45.274893Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MedNeXt-B\n",
      "Total parameters: 10,530,325\n",
      "Trainable parameters: 10,530,325\n",
      "Deep supervision: True\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Create MedNeXt model\n",
    "model = create_mednext_v1(\n",
    "    num_input_channels=Config.IN_CHANNELS,\n",
    "    num_classes=Config.NUM_CLASSES,\n",
    "    model_id=Config.MODEL_SIZE,\n",
    "    kernel_size=Config.KERNEL_SIZE,\n",
    "    deep_supervision=Config.DEEP_SUPERVISION\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(Config.DEVICE)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model: MedNeXt-{Config.MODEL_SIZE}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Deep supervision: {Config.DEEP_SUPERVISION}\")\n",
    "print(f\"Device: {Config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:20:45.440844Z",
     "iopub.status.busy": "2026-02-02T11:20:45.440564Z",
     "iopub.status.idle": "2026-02-02T11:20:46.816929Z",
     "shell.execute_reply": "2026-02-02T11:20:46.816228Z",
     "shell.execute_reply.started": "2026-02-02T11:20:45.440808Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep supervision outputs: 5\n",
      "  Output 0: torch.Size([1, 4, 128, 128, 128])\n",
      "  Output 1: torch.Size([1, 4, 64, 64, 64])\n",
      "  Output 2: torch.Size([1, 4, 32, 32, 32])\n",
      "  Output 3: torch.Size([1, 4, 16, 16, 16])\n",
      "  Output 4: torch.Size([1, 4, 8, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# Verify model with a forward pass\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(1, 4, 128, 128, 128).to(Config.DEVICE)\n",
    "    test_output = model(test_input)\n",
    "    \n",
    "    if isinstance(test_output, (list, tuple)):\n",
    "        print(f\"Deep supervision outputs: {len(test_output)}\")\n",
    "        for i, out in enumerate(test_output):\n",
    "            print(f\"  Output {i}: {out.shape}\")\n",
    "    else:\n",
    "        print(f\"Output shape: {test_output.shape}\")\n",
    "\n",
    "del test_input, test_output\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:20:46.818115Z",
     "iopub.status.busy": "2026-02-02T11:20:46.817813Z",
     "iopub.status.idle": "2026-02-02T11:20:50.445933Z",
     "shell.execute_reply": "2026-02-02T11:20:50.445109Z",
     "shell.execute_reply.started": "2026-02-02T11:20:46.818092Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete!\n",
      "Optimizer: AdamW (lr=0.0001, weight_decay=1e-05)\n",
      "Scheduler: CosineAnnealingWarmRestarts\n",
      "Mixed Precision: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/3755703013.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=Config.USE_AMP)\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "base_loss = CombinedLoss(dice_weight=1.0, ce_weight=1.0, include_background=False)\n",
    "\n",
    "if Config.DEEP_SUPERVISION:\n",
    "    criterion = DeepSupervisionLoss(base_loss)\n",
    "else:\n",
    "    criterion = base_loss\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=Config.LEARNING_RATE,\n",
    "    weight_decay=Config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Learning rate scheduler (Cosine Annealing with Warm Restarts)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,  # Restart every 10 epochs\n",
    "    T_mult=2,  # Double the restart period each time\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler(enabled=Config.USE_AMP)\n",
    "\n",
    "print(\"Training setup complete!\")\n",
    "print(f\"Optimizer: AdamW (lr={Config.LEARNING_RATE}, weight_decay={Config.WEIGHT_DECAY})\")\n",
    "print(f\"Scheduler: CosineAnnealingWarmRestarts\")\n",
    "print(f\"Mixed Precision: {Config.USE_AMP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:20:50.447404Z",
     "iopub.status.busy": "2026-02-02T11:20:50.446987Z",
     "iopub.status.idle": "2026-02-02T11:20:50.463998Z",
     "shell.execute_reply": "2026-02-02T11:20:50.463148Z",
     "shell.execute_reply.started": "2026-02-02T11:20:50.447383Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler: GradScaler,\n",
    "    device: torch.device,\n",
    "    epoch: int\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    all_dice_scores = []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")\n",
    "    \n",
    "    for batch_idx, (volumes, targets) in enumerate(pbar):\n",
    "        volumes = volumes.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with autocast(enabled=Config.USE_AMP):\n",
    "            outputs = model(volumes)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Compute Dice scores (use first output if deep supervision)\n",
    "        with torch.no_grad():\n",
    "            if isinstance(outputs, (list, tuple)):\n",
    "                pred = outputs[0]\n",
    "            else:\n",
    "                pred = outputs\n",
    "            dice = compute_dice_score(pred, targets, num_classes=Config.NUM_CLASSES)\n",
    "            all_dice_scores.append(dice)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'dice_mean': f\"{dice['Mean']:.4f}\"\n",
    "        })\n",
    "    \n",
    "    # Compute average metrics\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_dice = {\n",
    "        key: np.mean([d[key] for d in all_dice_scores]) \n",
    "        for key in all_dice_scores[0].keys()\n",
    "    }\n",
    "    \n",
    "    return {'loss': avg_loss, **avg_dice}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(\n",
    "    model: nn.Module,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    epoch: int\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    all_dice_scores = []\n",
    "    \n",
    "    pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\")\n",
    "    \n",
    "    for volumes, targets in pbar:\n",
    "        volumes = volumes.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        with autocast(enabled=Config.USE_AMP):\n",
    "            outputs = model(volumes)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Compute Dice scores\n",
    "        if isinstance(outputs, (list, tuple)):\n",
    "            pred = outputs[0]\n",
    "        else:\n",
    "            pred = outputs\n",
    "        dice = compute_dice_score(pred, targets, num_classes=Config.NUM_CLASSES)\n",
    "        all_dice_scores.append(dice)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'dice_mean': f\"{dice['Mean']:.4f}\"\n",
    "        })\n",
    "    \n",
    "    # Compute average metrics\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    avg_dice = {\n",
    "        key: np.mean([d[key] for d in all_dice_scores]) \n",
    "        for key in all_dice_scores[0].keys()\n",
    "    }\n",
    "    \n",
    "    return {'loss': avg_loss, **avg_dice}\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler,\n",
    "    scaler: GradScaler,\n",
    "    epoch: int,\n",
    "    metrics: Dict[str, float],\n",
    "    filepath: str\n",
    "):\n",
    "    \"\"\"Save a training checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(),\n",
    "        'metrics': metrics,\n",
    "        'config': {\n",
    "            'model_size': Config.MODEL_SIZE,\n",
    "            'kernel_size': Config.KERNEL_SIZE,\n",
    "            'in_channels': Config.IN_CHANNELS,\n",
    "            'num_classes': Config.NUM_CLASSES,\n",
    "            'deep_supervision': Config.DEEP_SUPERVISION,\n",
    "            'patch_size': Config.PATCH_SIZE\n",
    "        }\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved: {filepath}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler,\n",
    "    scaler: GradScaler,\n",
    "    filepath: str\n",
    ") -> int:\n",
    "    \"\"\"Load a training checkpoint.\"\"\"\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    print(f\"Checkpoint loaded: {filepath}\")\n",
    "    print(f\"Resuming from epoch {checkpoint['epoch'] + 1}\")\n",
    "    return checkpoint['epoch']\n",
    "\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:20:50.465411Z",
     "iopub.status.busy": "2026-02-02T11:20:50.465126Z",
     "iopub.status.idle": "2026-02-02T11:20:50.495011Z",
     "shell.execute_reply": "2026-02-02T11:20:50.494283Z",
     "shell.execute_reply.started": "2026-02-02T11:20:50.465387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_dice_mean': [],\n",
    "    'val_dice_mean': [],\n",
    "    'train_dice_wt': [],\n",
    "    'val_dice_wt': [],\n",
    "    'train_dice_tc': [],\n",
    "    'val_dice_tc': [],\n",
    "    'train_dice_et': [],\n",
    "    'val_dice_et': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_val_dice = 0.0\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "start_epoch = 0\n",
    "\n",
    "# Optional: Resume from checkpoint\n",
    "resume_training = False\n",
    "\n",
    "if resume_training and os.path.exists(checkpoint_path):\n",
    "    start_epoch = load_checkpoint(model, optimizer, scheduler, scaler, checkpoint_path) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:33:12.562798Z",
     "iopub.status.busy": "2026-02-02T11:33:12.562161Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting Training\n",
      "============================================================\n",
      "Improvements enabled:\n",
      "  ✓ Region-based loss (WT, TC, ET)\n",
      "  ✓ Gradient accumulation (effective batch size: 1)\n",
      "  ✓ TorchIO augmentations\n",
      "  ✓ Kernel size: 3x3x3\n",
      "============================================================\n",
      "\n",
      "Epoch 1/20 | LR: 1.00e-04\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|          | 0/2000 [00:00<?, ?it/s]/tmp/ipykernel_55/279245259.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=Config.USE_AMP):\n",
      "Epoch 1 [Train]:  13%|█▎        | 256/2000 [03:31<23:32,  1.23it/s, loss=0.4500, dice_mean=0.5893]"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Improvements enabled:\")\n",
    "print(f\"  ✓ Region-based loss (WT, TC, ET)\")\n",
    "print(f\"  ✓ Gradient accumulation (effective batch size: {Config.BATCH_SIZE * Config.ACCUMULATION_STEPS})\")\n",
    "print(f\"  ✓ TorchIO augmentations\")\n",
    "print(f\"  ✓ Kernel size: {Config.KERNEL_SIZE}x{Config.KERNEL_SIZE}x{Config.KERNEL_SIZE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(start_epoch, Config.NUM_EPOCHS):\n",
    "    # Get current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"\\nEpoch {epoch + 1}/{Config.NUM_EPOCHS} | LR: {current_lr:.2e}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train with gradient accumulation\n",
    "    train_metrics = train_one_epoch(\n",
    "        model, train_loader, criterion, optimizer, scaler, Config.DEVICE, epoch\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_metrics = validate(\n",
    "        model, val_loader, criterion, Config.DEVICE, epoch\n",
    "    )\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log metrics\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['train_dice_mean'].append(train_metrics['Mean'])\n",
    "    history['val_dice_mean'].append(val_metrics['Mean'])\n",
    "    history['train_dice_wt'].append(train_metrics['WT'])\n",
    "    history['val_dice_wt'].append(val_metrics['WT'])\n",
    "    history['train_dice_tc'].append(train_metrics['TC'])\n",
    "    history['val_dice_tc'].append(val_metrics['TC'])\n",
    "    history['train_dice_et'].append(train_metrics['ET'])\n",
    "    history['val_dice_et'].append(val_metrics['ET'])\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_metrics['loss']:.4f} | Val Loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"  Train Dice (Mean): {train_metrics['Mean']:.4f} | Val Dice (Mean): {val_metrics['Mean']:.4f}\")\n",
    "    print(f\"  Val Dice - WT: {val_metrics['WT']:.4f}, TC: {val_metrics['TC']:.4f}, ET: {val_metrics['ET']:.4f}\")\n",
    "    \n",
    "    # Save latest checkpoint\n",
    "    save_checkpoint(\n",
    "        model, optimizer, scheduler, scaler, epoch, val_metrics,\n",
    "        os.path.join(Config.CHECKPOINT_DIR, \"latest_checkpoint.pt\")\n",
    "    )\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['Mean'] > best_val_dice:\n",
    "        best_val_dice = val_metrics['Mean']\n",
    "        patience_counter = 0\n",
    "        save_checkpoint(\n",
    "            model, optimizer, scheduler, scaler, epoch, val_metrics,\n",
    "            os.path.join(Config.CHECKPOINT_DIR, \"best_model.pt\")\n",
    "        )\n",
    "        print(f\"  *** New best model! Val Dice Mean: {best_val_dice:.4f} ***\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement for {patience_counter}/{patience} epochs\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch + 1} epochs!\")\n",
    "        break\n",
    "    \n",
    "    # Save periodic checkpoints\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        save_checkpoint(\n",
    "            model, optimizer, scheduler, scaler, epoch, val_metrics,\n",
    "            os.path.join(Config.CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "        )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Best Validation Dice Mean: {best_val_dice:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# ============== Save Best Model for Deployment ==============\n",
    "best_model_path = os.path.join(Config.CHECKPOINT_DIR, \"best_model.pt\")\n",
    "if os.path.exists(best_model_path):\n",
    "    # Load the best model\n",
    "    best_checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Save model weights only (for easy deployment/inference elsewhere)\n",
    "    deployment_path = os.path.join(Config.CHECKPOINT_DIR, \"mednext_brats2021_weights.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'metrics': best_checkpoint['metrics'],\n",
    "        'epoch': best_checkpoint['epoch'],\n",
    "        'config': {\n",
    "            'model_size': Config.MODEL_SIZE,\n",
    "            'kernel_size': Config.KERNEL_SIZE,\n",
    "            'in_channels': Config.IN_CHANNELS,\n",
    "            'num_classes': Config.NUM_CLASSES,\n",
    "            'deep_supervision': Config.DEEP_SUPERVISION,\n",
    "            'patch_size': Config.PATCH_SIZE\n",
    "        }\n",
    "    }, deployment_path)\n",
    "    \n",
    "    print(f\"\\n✓ Best model saved for deployment: {deployment_path}\")\n",
    "    print(f\"  Best epoch: {best_checkpoint['epoch'] + 1}\")\n",
    "    print(f\"  Val Dice Mean: {best_checkpoint['metrics']['Mean']:.4f}\")\n",
    "else:\n",
    "    print(\"\\nWarning: No best model checkpoint found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-02T11:20:50.528673Z",
     "iopub.status.idle": "2026-02-02T11:20:50.528991Z",
     "shell.execute_reply": "2026-02-02T11:20:50.528872Z",
     "shell.execute_reply.started": "2026-02-02T11:20:50.528843Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss curves\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['train_loss'], label='Train Loss', color='blue')\n",
    "ax1.plot(history['val_loss'], label='Val Loss', color='red')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Mean Dice curves\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history['train_dice_mean'], label='Train Dice', color='blue')\n",
    "ax2.plot(history['val_dice_mean'], label='Val Dice', color='red')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Dice Score')\n",
    "ax2.set_title('Mean Dice Score')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# BraTS region Dice curves (Validation)\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(history['val_dice_wt'], label='Whole Tumor (WT)', color='green')\n",
    "ax3.plot(history['val_dice_tc'], label='Tumor Core (TC)', color='orange')\n",
    "ax3.plot(history['val_dice_et'], label='Enhancing Tumor (ET)', color='purple')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Dice Score')\n",
    "ax3.set_title('Validation Dice by Region')\n",
    "ax3.legend()\n",
    "ax3.grid(True)\n",
    "\n",
    "# Learning rate\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(history['lr'], label='Learning Rate', color='green')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Learning Rate')\n",
    "ax4.set_title('Learning Rate Schedule')\n",
    "ax4.set_yscale('log')\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Config.CHECKPOINT_DIR, 'training_curves.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining curves saved to {Config.CHECKPOINT_DIR}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== INFERENCE CONFIGURATION (STANDALONE) ==============\n",
    "# This section is completely independent and can run without the training code above\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Add MedNeXt to path (adjust if needed)\n",
    "sys.path.insert(0, 'mednext')\n",
    "from nnunet_mednext import create_mednext_v1, MedNeXt\n",
    "\n",
    "# ============== CHANGE THESE PATHS ==============\n",
    "MODEL_PATH = \"/kaggle/working/checkpoints/best_model.pt\"  # Path to your trained model\n",
    "DATASET_PATH = \"/kaggle/input/instant-odc-ai-hackathon/test\"  # Path to dataset for inference\n",
    "OUTPUT_CSV_PATH = \"submission.csv\"  # Path to save RLE results\n",
    "\n",
    "# Number of samples to VISUALIZE (visualization only, not inference)\n",
    "NUM_SAMPLES_TO_VISUALIZE = 5\n",
    "\n",
    "# ============== MODEL CONFIGURATION ==============\n",
    "class InferenceConfig:\n",
    "    # Model configuration (must match training)\n",
    "    MODEL_SIZE = 'B'          # 'S' (Small), 'B' (Base), 'M' (Medium), 'L' (Large)\n",
    "    KERNEL_SIZE = 3           # 3 or 5\n",
    "    IN_CHANNELS = 4           # 4 MRI modalities (T1, T1ce, T2, FLAIR)\n",
    "    NUM_CLASSES = 4           # Background + 3 tumor regions\n",
    "    \n",
    "    # Inference settings\n",
    "    PATCH_SIZE = (128, 128, 128)\n",
    "    USE_AMP = True\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Model path: {MODEL_PATH}\")\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"Output CSV: {OUTPUT_CSV_PATH}\")\n",
    "print(f\"Device: {InferenceConfig.DEVICE}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== RLE ENCODING FUNCTIONS ==============\n",
    "\n",
    "def rle_encode(mask):\n",
    "    \"\"\"\n",
    "    Run-Length Encoding for binary segmentation masks.\n",
    "    Produces format: \"start1 length1 start2 length2 ...\"\n",
    "    \n",
    "    Args:\n",
    "        mask: Binary segmentation mask (numpy array), 1 for foreground\n",
    "    \n",
    "    Returns:\n",
    "        RLE encoded string in format \"start length start length ...\"\n",
    "    \"\"\"\n",
    "    # Flatten in column-major (Fortran) order - standard for RLE competitions\n",
    "    pixels = mask.flatten(order='F')\n",
    "    \n",
    "    # Pad with zeros to detect transitions at boundaries\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    \n",
    "    # Find transition points\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1  # 1-indexed\n",
    "    \n",
    "    # Extract start positions and end positions\n",
    "    runs_starts = runs[0::2]  # Start of each run\n",
    "    runs_ends = runs[1::2]    # End of each run\n",
    "    \n",
    "    # Calculate lengths\n",
    "    runs_lengths = runs_ends - runs_starts\n",
    "    \n",
    "    # Build output string\n",
    "    rle_pairs = []\n",
    "    for start, length in zip(runs_starts, runs_lengths):\n",
    "        rle_pairs.extend([start, length])\n",
    "    \n",
    "    return ' '.join(str(x) for x in rle_pairs)\n",
    "\n",
    "\n",
    "def rle_encode_tumor(mask):\n",
    "    \"\"\"\n",
    "    RLE encode all tumor regions (non-background) as a single binary mask.\n",
    "    \n",
    "    Args:\n",
    "        mask: Multi-class segmentation mask (numpy array)\n",
    "              0 = background, 1/2/3 = tumor classes\n",
    "    \n",
    "    Returns:\n",
    "        RLE encoded string for all tumor regions combined\n",
    "    \"\"\"\n",
    "    # Combine all tumor classes into single binary mask\n",
    "    tumor_mask = (mask > 0).astype(np.uint8)\n",
    "    return rle_encode(tumor_mask)\n",
    "\n",
    "\n",
    "print(\"✅ RLE encoding functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== LOAD MODEL FOR INFERENCE ==============\n",
    "\n",
    "def load_model_for_inference(model_path, config):\n",
    "    \"\"\"\n",
    "    Load the trained MedNeXt model for inference.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model checkpoint\n",
    "        config: InferenceConfig object with model settings\n",
    "    \n",
    "    Returns:\n",
    "        Loaded model in evaluation mode\n",
    "    \"\"\"\n",
    "    # Load checkpoint first to check if it was trained with deep supervision\n",
    "    # Using weights_only=False for PyTorch 2.6+ compatibility\n",
    "    checkpoint = torch.load(model_path, map_location=config.DEVICE, weights_only=False)\n",
    "    \n",
    "    # Check if model was trained with deep supervision by looking for deep supervision keys\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    elif isinstance(checkpoint, dict):\n",
    "        state_dict = checkpoint\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Check if deep supervision was used during training\n",
    "    has_deep_supervision = any('out_1' in key or 'out_2' in key or 'out_3' in key or 'out_4' in key for key in state_dict.keys())\n",
    "    \n",
    "    print(f\"Checkpoint trained with deep supervision: {has_deep_supervision}\")\n",
    "    \n",
    "    # Create model with same configuration as training\n",
    "    # Match the deep_supervision setting from training\n",
    "    model = create_mednext_v1(\n",
    "        num_input_channels=config.IN_CHANNELS,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        model_id=config.MODEL_SIZE,\n",
    "        kernel_size=config.KERNEL_SIZE,\n",
    "        deep_supervision=has_deep_supervision  # Match training setting\n",
    "    )\n",
    "    \n",
    "    # Load the state dict\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    if isinstance(checkpoint, dict):\n",
    "        if 'epoch' in checkpoint:\n",
    "            print(f\"Loaded model from epoch {checkpoint.get('epoch', 'N/A')}\")\n",
    "        if 'best_val_dice' in checkpoint:\n",
    "            print(f\"Best validation Dice: {checkpoint.get('best_val_dice'):.4f}\")\n",
    "        if 'metrics' in checkpoint:\n",
    "            print(f\"Checkpoint metrics: {checkpoint['metrics']}\")\n",
    "    \n",
    "    model = model.to(config.DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded successfully from: {model_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load the model\n",
    "inference_model = load_model_for_inference(MODEL_PATH, InferenceConfig)\n",
    "print(f\"\\nModel is on device: {next(inference_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== INFERENCE DATASET ==============\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for inference - loads full volumes without patch extraction.\n",
    "    Completely standalone - no dependencies on training code.\n",
    "    Auto-detects file naming patterns for flexibility.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str, subject_ids: List[str] = None):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # Possible modality suffixes (will auto-detect)\n",
    "        self.modality_patterns = [\n",
    "            ['t1', 't1ce', 't2', 'flair'],           # lowercase with underscore\n",
    "            ['T1', 'T1ce', 'T2', 'FLAIR'],           # uppercase\n",
    "            ['T1', 'T1CE', 'T2', 'FLAIR'],           # uppercase CE\n",
    "            ['t1', 't1Gd', 't2', 'flair'],           # alternative Gd naming\n",
    "        ]\n",
    "        \n",
    "        # Get subject IDs if not provided\n",
    "        if subject_ids is None:\n",
    "            # Try to find subject folders with different patterns\n",
    "            if os.path.exists(data_dir):\n",
    "                all_items = os.listdir(data_dir)\n",
    "                self.subject_ids = sorted([\n",
    "                    d for d in all_items \n",
    "                    if os.path.isdir(os.path.join(data_dir, d))\n",
    "                ])\n",
    "            else:\n",
    "                self.subject_ids = []\n",
    "        else:\n",
    "            self.subject_ids = subject_ids\n",
    "        \n",
    "        print(f\"Found {len(self.subject_ids)} subjects in dataset\")\n",
    "        if len(self.subject_ids) > 0:\n",
    "            print(f\"Sample subjects: {self.subject_ids[:3]}\")\n",
    "            # Show files in first subject folder for debugging\n",
    "            first_subject_dir = os.path.join(data_dir, self.subject_ids[0])\n",
    "            if os.path.exists(first_subject_dir):\n",
    "                files = os.listdir(first_subject_dir)\n",
    "                print(f\"Files in first subject folder: {files}\")\n",
    "        \n",
    "        # Detect the modality pattern from the first subject\n",
    "        self.modalities = self._detect_modality_pattern()\n",
    "        print(f\"Detected modalities: {self.modalities}\")\n",
    "    \n",
    "    def _detect_modality_pattern(self):\n",
    "        \"\"\"Auto-detect the modality file naming pattern.\"\"\"\n",
    "        if len(self.subject_ids) == 0:\n",
    "            return ['t1', 't1ce', 't2', 'flair']  # default\n",
    "        \n",
    "        subject_id = self.subject_ids[0]\n",
    "        subject_dir = os.path.join(self.data_dir, subject_id)\n",
    "        \n",
    "        if not os.path.exists(subject_dir):\n",
    "            return ['t1', 't1ce', 't2', 'flair']  # default\n",
    "        \n",
    "        files = os.listdir(subject_dir)\n",
    "        \n",
    "        # Try each pattern\n",
    "        for pattern in self.modality_patterns:\n",
    "            # Check with underscore separator\n",
    "            found_all = True\n",
    "            for mod in pattern:\n",
    "                # Try different separators and extensions\n",
    "                possible_names = [\n",
    "                    f\"{subject_id}_{mod}.nii.gz\",\n",
    "                    f\"{subject_id}-{mod}.nii.gz\",\n",
    "                    f\"{subject_id}_{mod}.nii\",\n",
    "                    f\"{mod}.nii.gz\",\n",
    "                    f\"{mod}.nii\",\n",
    "                ]\n",
    "                if not any(name in files for name in possible_names):\n",
    "                    found_all = False\n",
    "                    break\n",
    "            \n",
    "            if found_all:\n",
    "                return pattern\n",
    "        \n",
    "        # If no pattern matched, try to detect from actual files\n",
    "        detected = []\n",
    "        modality_keywords = {\n",
    "            't1ce': ['t1ce', 't1gd', 't1Gd', 'T1CE', 'T1Gd', 'T1ce'],\n",
    "            't1': ['t1', 'T1'],\n",
    "            't2': ['t2', 'T2'],\n",
    "            'flair': ['flair', 'FLAIR', 'Flair'],\n",
    "        }\n",
    "        \n",
    "        for mod_key, keywords in modality_keywords.items():\n",
    "            for f in files:\n",
    "                f_lower = f.lower()\n",
    "                if any(kw.lower() in f_lower for kw in keywords):\n",
    "                    # Extract the actual modality name from file\n",
    "                    for kw in keywords:\n",
    "                        if kw in f:\n",
    "                            detected.append(kw)\n",
    "                            break\n",
    "                    break\n",
    "        \n",
    "        if len(detected) == 4:\n",
    "            return detected\n",
    "        \n",
    "        return ['t1', 't1ce', 't2', 'flair']  # default fallback\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.subject_ids)\n",
    "    \n",
    "    def _find_modality_file(self, subject_dir: str, subject_id: str, modality: str) -> str:\n",
    "        \"\"\"Find the file for a given modality, trying different naming conventions.\"\"\"\n",
    "        files = os.listdir(subject_dir)\n",
    "        \n",
    "        # Try different naming patterns\n",
    "        patterns = [\n",
    "            f\"{subject_id}_{modality}.nii.gz\",\n",
    "            f\"{subject_id}-{modality}.nii.gz\",\n",
    "            f\"{subject_id}_{modality}.nii\",\n",
    "            f\"{modality}.nii.gz\",\n",
    "            f\"{modality}.nii\",\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            if pattern in files:\n",
    "                return os.path.join(subject_dir, pattern)\n",
    "        \n",
    "        # Try case-insensitive search\n",
    "        modality_lower = modality.lower()\n",
    "        for f in files:\n",
    "            if modality_lower in f.lower() and ('nii' in f.lower()):\n",
    "                # Make sure it's the right modality (not t1 matching t1ce)\n",
    "                if modality_lower == 't1':\n",
    "                    if 't1ce' not in f.lower() and 't1gd' not in f.lower():\n",
    "                        return os.path.join(subject_dir, f)\n",
    "                else:\n",
    "                    return os.path.join(subject_dir, f)\n",
    "        \n",
    "        raise FileNotFoundError(f\"Could not find {modality} file in {subject_dir}. Available files: {files}\")\n",
    "    \n",
    "    def _load_nifti(self, filepath: str) -> np.ndarray:\n",
    "        \"\"\"Load a NIfTI file and return the data array.\"\"\"\n",
    "        img = nib.load(filepath)\n",
    "        return img.get_fdata().astype(np.float32)\n",
    "    \n",
    "    def _normalize(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Z-score normalization per volume (non-zero voxels only).\"\"\"\n",
    "        mask = data > 0\n",
    "        if mask.sum() > 0:\n",
    "            mean = data[mask].mean()\n",
    "            std = data[mask].std()\n",
    "            if std > 0:\n",
    "                data = (data - mean) / std\n",
    "                data[~mask] = 0\n",
    "        return data\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        subject_id = self.subject_ids[idx]\n",
    "        subject_dir = os.path.join(self.data_dir, subject_id)\n",
    "        \n",
    "        # Load all modalities\n",
    "        modality_data = []\n",
    "        for mod in self.modalities:\n",
    "            filepath = self._find_modality_file(subject_dir, subject_id, mod)\n",
    "            data = self._load_nifti(filepath)\n",
    "            data = self._normalize(data)\n",
    "            modality_data.append(data)\n",
    "        \n",
    "        # Stack modalities: (4, D, H, W)\n",
    "        volume = np.stack(modality_data, axis=0)\n",
    "        \n",
    "        # Load ground truth segmentation if available\n",
    "        seg = None\n",
    "        seg_patterns = [\n",
    "            f\"{subject_id}_seg.nii.gz\",\n",
    "            f\"{subject_id}-seg.nii.gz\",\n",
    "            f\"seg.nii.gz\",\n",
    "            f\"{subject_id}_mask.nii.gz\",\n",
    "        ]\n",
    "        \n",
    "        files = os.listdir(subject_dir)\n",
    "        for pattern in seg_patterns:\n",
    "            if pattern in files:\n",
    "                seg_path = os.path.join(subject_dir, pattern)\n",
    "                seg = self._load_nifti(seg_path)\n",
    "                # Convert labels: 0, 1, 2, 4 -> 0, 1, 2, 3\n",
    "                new_seg = np.zeros_like(seg)\n",
    "                new_seg[seg == 0] = 0\n",
    "                new_seg[seg == 1] = 1\n",
    "                new_seg[seg == 2] = 2\n",
    "                new_seg[seg == 4] = 3\n",
    "                seg = new_seg\n",
    "                break\n",
    "        \n",
    "        volume_tensor = torch.from_numpy(volume.copy()).float()\n",
    "        \n",
    "        return {\n",
    "            'subject_id': subject_id,\n",
    "            'volume': volume_tensor,\n",
    "            'ground_truth': seg,\n",
    "            'original_shape': volume.shape[1:]\n",
    "        }\n",
    "\n",
    "\n",
    "# Create inference dataset\n",
    "inference_dataset = InferenceDataset(DATASET_PATH)\n",
    "print(f\"\\nInference dataset created with {len(inference_dataset)} subjects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== SLIDING WINDOW INFERENCE ==============\n",
    "\n",
    "def sliding_window_inference(model, volume, config, patch_size=(128, 128, 128), overlap=0.5):\n",
    "    \"\"\"\n",
    "    Perform sliding window inference for large 3D volumes.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        volume: Input volume tensor (C, D, H, W)\n",
    "        config: InferenceConfig object\n",
    "        patch_size: Size of patches for inference\n",
    "        overlap: Overlap ratio between patches\n",
    "    \n",
    "    Returns:\n",
    "        Predicted segmentation mask\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = config.DEVICE\n",
    "    \n",
    "    C, D, H, W = volume.shape\n",
    "    pd, ph, pw = patch_size\n",
    "    \n",
    "    # Calculate stride\n",
    "    stride_d = int(pd * (1 - overlap))\n",
    "    stride_h = int(ph * (1 - overlap))\n",
    "    stride_w = int(pw * (1 - overlap))\n",
    "    \n",
    "    # Pad volume if necessary\n",
    "    pad_d = max(0, pd - D)\n",
    "    pad_h = max(0, ph - H)\n",
    "    pad_w = max(0, pw - W)\n",
    "    \n",
    "    if pad_d > 0 or pad_h > 0 or pad_w > 0:\n",
    "        volume = F.pad(volume, (0, pad_w, 0, pad_h, 0, pad_d))\n",
    "        D, H, W = volume.shape[1:]\n",
    "    \n",
    "    # Initialize output and count tensors\n",
    "    output = torch.zeros((config.NUM_CLASSES, D, H, W), device=device)\n",
    "    count = torch.zeros((D, H, W), device=device)\n",
    "    \n",
    "    # Generate patch positions\n",
    "    d_positions = list(range(0, max(1, D - pd + 1), stride_d))\n",
    "    h_positions = list(range(0, max(1, H - ph + 1), stride_h))\n",
    "    w_positions = list(range(0, max(1, W - pw + 1), stride_w))\n",
    "    \n",
    "    # Ensure we cover the entire volume\n",
    "    if D > pd and D - pd not in d_positions:\n",
    "        d_positions.append(D - pd)\n",
    "    if H > ph and H - ph not in h_positions:\n",
    "        h_positions.append(H - ph)\n",
    "    if W > pw and W - pw not in w_positions:\n",
    "        w_positions.append(W - pw)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d_start in d_positions:\n",
    "            for h_start in h_positions:\n",
    "                for w_start in w_positions:\n",
    "                    # Extract patch\n",
    "                    patch = volume[:, d_start:d_start+pd, h_start:h_start+ph, w_start:w_start+pw]\n",
    "                    patch = patch.unsqueeze(0).to(device)  # Add batch dimension\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    with torch.cuda.amp.autocast(enabled=config.USE_AMP):\n",
    "                        pred = model(patch)\n",
    "                    \n",
    "                    # Handle deep supervision output (returns list of tensors)\n",
    "                    if isinstance(pred, (list, tuple)):\n",
    "                        pred = pred[0]  # Use the first (full resolution) output\n",
    "                    \n",
    "                    pred = F.softmax(pred, dim=1).squeeze(0)  # Remove batch dimension\n",
    "                    \n",
    "                    # Accumulate predictions\n",
    "                    output[:, d_start:d_start+pd, h_start:h_start+ph, w_start:w_start+pw] += pred\n",
    "                    count[d_start:d_start+pd, h_start:h_start+ph, w_start:w_start+pw] += 1\n",
    "    \n",
    "    # Average predictions\n",
    "    output = output / count.unsqueeze(0).clamp(min=1)\n",
    "    \n",
    "    # Remove padding\n",
    "    original_d = D - pad_d if pad_d > 0 else D\n",
    "    original_h = H - pad_h if pad_h > 0 else H\n",
    "    original_w = W - pad_w if pad_w > 0 else W\n",
    "    output = output[:, :original_d, :original_h, :original_w]\n",
    "    \n",
    "    # Get final prediction\n",
    "    prediction = torch.argmax(output, dim=0).cpu().numpy()\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "\n",
    "print(\"Sliding window inference function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== RUN INFERENCE ON ALL SAMPLES ==============\n",
    "\n",
    "def run_inference_for_submission(model, dataset, config):\n",
    "    \"\"\"\n",
    "    Run inference on ALL samples and generate RLE encodings.\n",
    "    Stores only what's needed to minimize memory usage.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        dataset: Inference dataset\n",
    "        config: InferenceConfig object\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with subject_id and rle_encoding\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    num_samples = len(dataset)\n",
    "    \n",
    "    print(f\"Running inference on {num_samples} samples...\")\n",
    "    \n",
    "    for idx in tqdm(range(num_samples)):\n",
    "        sample = dataset[idx]\n",
    "        subject_id = sample['subject_id']\n",
    "        volume = sample['volume']\n",
    "        \n",
    "        # Run sliding window inference\n",
    "        prediction = sliding_window_inference(\n",
    "            model, \n",
    "            volume, \n",
    "            config,\n",
    "            patch_size=config.PATCH_SIZE,\n",
    "            overlap=0.5\n",
    "        )\n",
    "        \n",
    "        # RLE encode the tumor mask (all non-background classes combined)\n",
    "        rle_encoding = rle_encode_tumor(prediction)\n",
    "        \n",
    "        # Store result\n",
    "        result = {\n",
    "            'subject_id': subject_id,\n",
    "            'rle_encoding': rle_encoding,\n",
    "        }\n",
    "        \n",
    "        # Store full data only for visualization samples\n",
    "        if idx < NUM_SAMPLES_TO_VISUALIZE:\n",
    "            result['prediction'] = prediction\n",
    "            result['volume'] = volume.numpy()\n",
    "            result['ground_truth'] = sample['ground_truth']\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Clear GPU cache periodically\n",
    "        if idx % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"  {subject_id}: unique labels = {np.unique(prediction)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ============== RUN INFERENCE ==============\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNNING INFERENCE ON ENTIRE DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "inference_results = run_inference_for_submission(\n",
    "    inference_model, \n",
    "    inference_dataset, \n",
    "    InferenceConfig\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"✅ Inference complete! Processed {len(inference_results)} samples\")\n",
    "print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== SAVE RLE RESULTS TO CSV ==============\n",
    "\n",
    "def save_submission(results, output_path=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Save RLE encoded results to CSV file.\n",
    "    Format: Id<TAB>Expected\n",
    "    \n",
    "    Args:\n",
    "        results: List of inference results with 'subject_id' and 'rle_encoding'\n",
    "        output_path: Path to save the CSV file\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for result in results:\n",
    "        rows.append({\n",
    "            'Id': result['subject_id'],\n",
    "            'Expected': result['rle_encoding']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_path, index=False, sep='\\t')\n",
    "    \n",
    "    print(f\"✅ Submission saved to: {output_path}\")\n",
    "    print(f\"   Total samples: {len(df)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============== SAVE SUBMISSION ==============\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING SUBMISSION FILE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "submission_df = save_submission(inference_results, output_path=OUTPUT_CSV_PATH)\n",
    "\n",
    "print(f\"\\nSubmission file: {OUTPUT_CSV_PATH}\")\n",
    "print(f\"Shape: {submission_df.shape}\")\n",
    "\n",
    "# Show sample output\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE OUTPUT (first 3 rows, truncated RLE):\")\n",
    "print(\"=\" * 60)\n",
    "for idx, row in submission_df.head(3).iterrows():\n",
    "    rle_preview = row['Expected'][:100] + '...' if len(row['Expected']) > 100 else row['Expected']\n",
    "    print(f\"{row['Id']}\\t{rle_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== VISUALIZATION FUNCTIONS ==============\n",
    "\n",
    "def visualize_segmentation_slices(result, slice_indices=None, figsize=(20, 10)):\n",
    "    \"\"\"\n",
    "    Visualize segmentation results for a single subject.\n",
    "    Shows FLAIR image, ground truth, and prediction side by side.\n",
    "    \n",
    "    Args:\n",
    "        result: Dictionary containing prediction, ground truth, and volume\n",
    "        slice_indices: List of slice indices to visualize (axial slices)\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    subject_id = result['subject_id']\n",
    "    volume = result['volume']  # (4, D, H, W)\n",
    "    prediction = result['prediction']  # (D, H, W)\n",
    "    ground_truth = result['ground_truth']  # (D, H, W) or None\n",
    "    \n",
    "    # Use FLAIR modality for visualization (index 3)\n",
    "    flair = volume[3]  # (D, H, W)\n",
    "    \n",
    "    # Auto-select slices if not provided (pick slices with tumor)\n",
    "    if slice_indices is None:\n",
    "        if ground_truth is not None:\n",
    "            tumor_slices = np.where(ground_truth.sum(axis=(1, 2)) > 0)[0]\n",
    "        else:\n",
    "            tumor_slices = np.where(prediction.sum(axis=(1, 2)) > 0)[0]\n",
    "        \n",
    "        if len(tumor_slices) > 0:\n",
    "            # Pick 5 evenly spaced slices from tumor region\n",
    "            indices = np.linspace(0, len(tumor_slices) - 1, min(5, len(tumor_slices)), dtype=int)\n",
    "            slice_indices = tumor_slices[indices]\n",
    "        else:\n",
    "            # Fallback to middle slices\n",
    "            slice_indices = np.linspace(flair.shape[0] // 4, 3 * flair.shape[0] // 4, 5, dtype=int)\n",
    "    \n",
    "    num_slices = len(slice_indices)\n",
    "    has_gt = ground_truth is not None\n",
    "    num_rows = 3 if has_gt else 2\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_slices, figsize=figsize)\n",
    "    \n",
    "    # Color map for segmentation\n",
    "    # 0: Background (black), 1: NCR/NET (red), 2: ED (green), 3: ET (yellow)\n",
    "    colors = np.array([\n",
    "        [0, 0, 0],        # Background - black\n",
    "        [255, 0, 0],      # NCR/NET - red\n",
    "        [0, 255, 0],      # ED - green\n",
    "        [255, 255, 0]     # ET - yellow\n",
    "    ]) / 255.0\n",
    "    \n",
    "    def apply_colormap(mask):\n",
    "        \"\"\"Apply custom colormap to segmentation mask.\"\"\"\n",
    "        colored = np.zeros((*mask.shape, 3))\n",
    "        for i in range(4):\n",
    "            colored[mask == i] = colors[i]\n",
    "        return colored\n",
    "    \n",
    "    for col, slice_idx in enumerate(slice_indices):\n",
    "        # FLAIR image\n",
    "        axes[0, col].imshow(flair[slice_idx].T, cmap='gray', origin='lower')\n",
    "        axes[0, col].set_title(f'FLAIR (Slice {slice_idx})')\n",
    "        axes[0, col].axis('off')\n",
    "        \n",
    "        # Ground truth\n",
    "        if has_gt:\n",
    "            gt_colored = apply_colormap(ground_truth[slice_idx])\n",
    "            axes[1, col].imshow(flair[slice_idx].T, cmap='gray', origin='lower', alpha=0.7)\n",
    "            axes[1, col].imshow(gt_colored.transpose(1, 0, 2), origin='lower', alpha=0.5)\n",
    "            axes[1, col].set_title('Ground Truth')\n",
    "            axes[1, col].axis('off')\n",
    "        \n",
    "        # Prediction\n",
    "        pred_row = 2 if has_gt else 1\n",
    "        pred_colored = apply_colormap(prediction[slice_idx])\n",
    "        axes[pred_row, col].imshow(flair[slice_idx].T, cmap='gray', origin='lower', alpha=0.7)\n",
    "        axes[pred_row, col].imshow(pred_colored.transpose(1, 0, 2), origin='lower', alpha=0.5)\n",
    "        axes[pred_row, col].set_title('Prediction')\n",
    "        axes[pred_row, col].axis('off')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='red', markersize=10, label='NCR/NET'),\n",
    "        plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='green', markersize=10, label='ED'),\n",
    "        plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='yellow', markersize=10, label='ET')\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "    \n",
    "    plt.suptitle(f'Segmentation Results: {subject_id}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Visualization functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== VISUALIZE FIRST N SAMPLES (CONFIGURABLE) ==============\n",
    "\n",
    "# Only visualize the first NUM_SAMPLES_TO_VISUALIZE samples\n",
    "# Full data is stored for these samples, RLE is stored for ALL samples\n",
    "\n",
    "samples_to_visualize = min(NUM_SAMPLES_TO_VISUALIZE, len(inference_results))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"SEGMENTATION VISUALIZATION ({samples_to_visualize} of {len(inference_results)} samples)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nLegend:\")\n",
    "print(\"  🔴 NCR/NET (Necrotic and Non-Enhancing Tumor)\")\n",
    "print(\"  🟢 ED (Peritumoral Edema)\")\n",
    "print(\"  🟡 ET (GD-Enhancing Tumor)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNote: RLE encodings generated for ALL {len(inference_results)} samples\")\n",
    "print(f\"      Visualization shown for first {samples_to_visualize} samples only\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(samples_to_visualize):\n",
    "    result = inference_results[i]\n",
    "    if result['prediction'] is not None and result['volume'] is not None:\n",
    "        print(f\"\\n[{i+1}/{samples_to_visualize}] Visualizing: {result['subject_id']}\")\n",
    "        visualize_segmentation_slices(result, figsize=(18, 12))\n",
    "    else:\n",
    "        print(f\"\\n[{i+1}] {result['subject_id']}: Full data not stored for this sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== 3D VISUALIZATION (OPTIONAL) ==============\n",
    "\n",
    "def visualize_3d_segmentation(result, figsize=(16, 6)):\n",
    "    \"\"\"\n",
    "    Create a 3D-like visualization showing orthogonal views (axial, sagittal, coronal).\n",
    "    \n",
    "    Args:\n",
    "        result: Dictionary containing prediction, ground truth, and volume\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    subject_id = result['subject_id']\n",
    "    volume = result['volume']\n",
    "    prediction = result['prediction']\n",
    "    ground_truth = result['ground_truth']\n",
    "    \n",
    "    # Check if data is available\n",
    "    if volume is None or prediction is None:\n",
    "        print(f\"Skipping {subject_id}: Full data not stored for this sample\")\n",
    "        return\n",
    "    \n",
    "    flair = volume[3]  # Use FLAIR for background\n",
    "    \n",
    "    # Find center of tumor for slice selection\n",
    "    if ground_truth is not None:\n",
    "        tumor_mask = ground_truth > 0\n",
    "    else:\n",
    "        tumor_mask = prediction > 0\n",
    "    \n",
    "    if tumor_mask.sum() > 0:\n",
    "        coords = np.where(tumor_mask)\n",
    "        center_d = int(np.mean(coords[0]))\n",
    "        center_h = int(np.mean(coords[1]))\n",
    "        center_w = int(np.mean(coords[2]))\n",
    "    else:\n",
    "        center_d, center_h, center_w = [s // 2 for s in flair.shape]\n",
    "    \n",
    "    # Color map\n",
    "    colors = np.array([\n",
    "        [0, 0, 0],\n",
    "        [255, 0, 0],\n",
    "        [0, 255, 0],\n",
    "        [255, 255, 0]\n",
    "    ]) / 255.0\n",
    "    \n",
    "    def apply_colormap(mask):\n",
    "        colored = np.zeros((*mask.shape, 3))\n",
    "        for i in range(4):\n",
    "            colored[mask == i] = colors[i]\n",
    "        return colored\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
    "    \n",
    "    views = [\n",
    "        ('Axial', flair[center_d], prediction[center_d], ground_truth[center_d] if ground_truth is not None else None),\n",
    "        ('Sagittal', flair[:, :, center_w], prediction[:, :, center_w], ground_truth[:, :, center_w] if ground_truth is not None else None),\n",
    "        ('Coronal', flair[:, center_h, :], prediction[:, center_h, :], ground_truth[:, center_h, :] if ground_truth is not None else None)\n",
    "    ]\n",
    "    \n",
    "    for col, (view_name, img, pred, gt) in enumerate(views):\n",
    "        # Top row: Ground Truth (or FLAIR if no GT)\n",
    "        axes[0, col].imshow(img.T, cmap='gray', origin='lower', alpha=0.7)\n",
    "        if gt is not None:\n",
    "            gt_colored = apply_colormap(gt)\n",
    "            axes[0, col].imshow(gt_colored.transpose(1, 0, 2), origin='lower', alpha=0.5)\n",
    "            axes[0, col].set_title(f'{view_name} - Ground Truth')\n",
    "        else:\n",
    "            axes[0, col].set_title(f'{view_name} - FLAIR')\n",
    "        axes[0, col].axis('off')\n",
    "        \n",
    "        # Bottom row: Prediction\n",
    "        axes[1, col].imshow(img.T, cmap='gray', origin='lower', alpha=0.7)\n",
    "        pred_colored = apply_colormap(pred)\n",
    "        axes[1, col].imshow(pred_colored.transpose(1, 0, 2), origin='lower', alpha=0.5)\n",
    "        axes[1, col].set_title(f'{view_name} - Prediction')\n",
    "        axes[1, col].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Multi-View Segmentation: {subject_id}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize first sample in 3D views (only if data available)\n",
    "if len(inference_results) > 0:\n",
    "    first_result = inference_results[0]\n",
    "    if first_result['volume'] is not None and first_result['prediction'] is not None:\n",
    "        print(\"\\n3D Multi-View Visualization (First Sample):\")\n",
    "        visualize_3d_segmentation(first_result)\n",
    "    else:\n",
    "        print(\"First sample does not have full data stored for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== SUMMARY STATISTICS ==============\n",
    "\n",
    "def compute_dice_score_inference(pred, gt, num_classes=4):\n",
    "    \"\"\"Compute Dice score for each class.\"\"\"\n",
    "    dice_scores = {}\n",
    "    class_names = ['Background', 'NCR/NET', 'ED', 'ET']\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred == c).astype(float)\n",
    "        gt_c = (gt == c).astype(float)\n",
    "        \n",
    "        intersection = (pred_c * gt_c).sum()\n",
    "        union = pred_c.sum() + gt_c.sum()\n",
    "        \n",
    "        if union > 0:\n",
    "            dice = 2 * intersection / union\n",
    "        else:\n",
    "            dice = 1.0 if pred_c.sum() == 0 else 0.0\n",
    "        \n",
    "        dice_scores[class_names[c]] = dice\n",
    "    \n",
    "    return dice_scores\n",
    "\n",
    "\n",
    "# Compute metrics for all results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INFERENCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "metrics_list = []\n",
    "for result in inference_results:\n",
    "    subject_id = result['subject_id']\n",
    "    prediction = result['prediction']\n",
    "    ground_truth = result['ground_truth']\n",
    "    rle_encodings = result['rle_encodings']\n",
    "    \n",
    "    print(f\"\\n📊 {subject_id}\")\n",
    "    print(f\"   Prediction shape: {prediction.shape}\")\n",
    "    print(f\"   Unique classes predicted: {np.unique(prediction)}\")\n",
    "    \n",
    "    # RLE encoding info\n",
    "    class_names = ['Background', 'NCR/NET', 'ED', 'ET']\n",
    "    for class_idx, rle in rle_encodings.items():\n",
    "        rle_preview = rle[:50] + \"...\" if len(rle) > 50 else rle\n",
    "        print(f\"   RLE [{class_names[class_idx]}]: {rle_preview}\")\n",
    "    \n",
    "    # Compute Dice if ground truth available\n",
    "    if ground_truth is not None:\n",
    "        dice_scores = compute_dice_score_inference(prediction, ground_truth)\n",
    "        print(f\"   Dice Scores:\")\n",
    "        for name, score in dice_scores.items():\n",
    "            if name != 'Background':\n",
    "                print(f\"      - {name}: {score:.4f}\")\n",
    "        metrics_list.append({\n",
    "            'subject_id': subject_id,\n",
    "            **dice_scores\n",
    "        })\n",
    "\n",
    "# Summary statistics\n",
    "if metrics_list:\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OVERALL PERFORMANCE\")\n",
    "    print(\"=\" * 60)\n",
    "    for col in ['NCR/NET', 'ED', 'ET']:\n",
    "        mean_dice = metrics_df[col].mean()\n",
    "        std_dice = metrics_df[col].std()\n",
    "        print(f\"   {col}: {mean_dice:.4f} ± {std_dice:.4f}\")\n",
    "    \n",
    "    # Mean Dice (excluding background)\n",
    "    mean_overall = metrics_df[['NCR/NET', 'ED', 'ET']].mean(axis=1).mean()\n",
    "    print(f\"\\n   Overall Mean Dice: {mean_overall:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Inference pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1541666,
     "isSourceIdPinned": false,
     "sourceId": 2542390,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
