{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 3D U-Net Inference and Visualization\n",
        "\n",
        "This notebook loads a pre-trained 3D U-Net model and performs inference on brain tumor MRI data.\n",
        "\n",
        "## Contents\n",
        "1. Imports and Setup\n",
        "2. 3D U-Net Model Architecture\n",
        "3. Load Pre-trained Model\n",
        "4. Inference Functions\n",
        "5. Visualization\n",
        "6. Generate Submission\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1. Imports and Setup\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import nibabel as nib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {DEVICE}\")"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2. 3D U-Net Model Architecture\n",
        "\n",
        "The same architecture used during training must be defined here.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"Double convolution block: (Conv3D -> InstanceNorm -> LeakyReLU) x 2\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.InstanceNorm3d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.InstanceNorm3d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downsampling with MaxPool followed by DoubleConv.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool3d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upsampling followed by DoubleConv with skip connection.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose3d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet3D(nn.Module):\n",
        "    \"\"\"3D U-Net for volumetric segmentation.\"\"\"\n",
        "    def __init__(self, in_channels=4, out_channels=4, features=32):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Encoder\n",
        "        self.inc = DoubleConv(in_channels, features)\n",
        "        self.down1 = Down(features, features * 2)\n",
        "        self.down2 = Down(features * 2, features * 4)\n",
        "        self.down3 = Down(features * 4, features * 8)\n",
        "        self.down4 = Down(features * 8, features * 16)\n",
        "        \n",
        "        # Decoder\n",
        "        self.up1 = Up(features * 16, features * 8)\n",
        "        self.up2 = Up(features * 8, features * 4)\n",
        "        self.up3 = Up(features * 4, features * 2)\n",
        "        self.up4 = Up(features * 2, features)\n",
        "        \n",
        "        # Output\n",
        "        self.outc = nn.Conv3d(features, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        \n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        \n",
        "        return self.outc(x)"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3. Load Pre-trained Model\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH = \"/kaggle/input/unet3d-checkpoint/unet3d_best.pth\"\n",
        "\n",
        "# Initialize model\n",
        "model = UNet3D(in_channels=4, out_channels=4).to(DEVICE)\n",
        "\n",
        "# Load weights\n",
        "try:\n",
        "    state_dict = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
        "    model.load_state_dict(state_dict)\n",
        "    print(\"Model weights loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading weights: {e}\")\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4. Inference Functions\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_volume(volume):\n",
        "    \"\"\"Robust Z-Score Normalization.\"\"\"\n",
        "    mask = volume > 0\n",
        "    if np.sum(mask) == 0:\n",
        "        return volume\n",
        "\n",
        "    pixels = volume[mask]\n",
        "    p_low, p_high = np.percentile(pixels, 0.5), np.percentile(pixels, 99.5)\n",
        "    volume = np.clip(volume, p_low, p_high)\n",
        "    \n",
        "    pixels = volume[mask]\n",
        "    mean, std = pixels.mean(), pixels.std()\n",
        "    volume = (volume - mean) / (std + 1e-8)\n",
        "    volume[~mask] = 0\n",
        "    \n",
        "    return volume\n",
        "\n",
        "\n",
        "def sliding_window_inference(model, image, patch_size=(96, 96, 96), overlap=0.5):\n",
        "    \"\"\"\n",
        "    Sliding window inference for large 3D volumes.\n",
        "    Handles arbitrary batch sizes.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    batch_size = image.shape[0]\n",
        "    image_size = image.shape[2:]\n",
        "    \n",
        "    output_probs = torch.zeros((batch_size, 4, *image_size), device=image.device, dtype=torch.float32)\n",
        "    count_map = torch.zeros((batch_size, 4, *image_size), device=image.device, dtype=torch.float32)\n",
        "    \n",
        "    step = [int(p * (1 - overlap)) for p in patch_size]\n",
        "    \n",
        "    for z in range(0, image_size[0] - patch_size[0] + 1, step[0]):\n",
        "        for y in range(0, image_size[1] - patch_size[1] + 1, step[1]):\n",
        "            for x in range(0, image_size[2] - patch_size[2] + 1, step[2]):\n",
        "                patch = image[:, :, z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]]\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
        "                        pred = model(patch)\n",
        "                        pred = F.softmax(pred, dim=1)\n",
        "                \n",
        "                output_probs[:, :, z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]] += pred\n",
        "                count_map[:, :, z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]] += 1\n",
        "    \n",
        "    output_probs /= count_map.clamp(min=1)\n",
        "    return output_probs\n",
        "\n",
        "\n",
        "def simple_resize_inference(model, image, target_size=(96, 96, 96)):\n",
        "    \"\"\"\n",
        "    Simple resize-based inference for faster processing.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    original_shape = image.shape[2:]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        small_image = F.interpolate(image, size=target_size, mode='trilinear', align_corners=False)\n",
        "        \n",
        "        with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
        "            pred = model(small_image)\n",
        "        \n",
        "        pred = F.interpolate(pred, size=original_shape, mode='trilinear', align_corners=False)\n",
        "        pred = F.softmax(pred, dim=1)\n",
        "    \n",
        "    return pred"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 5. Visualization\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_test_prediction(model, test_path, patient_id=None):\n",
        "    \"\"\"\n",
        "    Visualize the model's prediction for a test patient.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Pick a patient\n",
        "    if patient_id is None:\n",
        "        patients = sorted([p for p in os.listdir(test_path) if os.path.isdir(os.path.join(test_path, p))])\n",
        "        if len(patients) == 0:\n",
        "            print(\"No patients found!\")\n",
        "            return\n",
        "        patient_id = patients[0]\n",
        "    \n",
        "    patient_path = os.path.join(test_path, patient_id)\n",
        "    print(f\"Processing patient: {patient_id}\")\n",
        "    \n",
        "    # Load modalities\n",
        "    modalities = {}\n",
        "    for f in os.listdir(patient_path):\n",
        "        if f.endswith('.nii.gz'):\n",
        "            for mod in ['t1ce', 't1', 't2', 'flair']:\n",
        "                if mod in f.lower():\n",
        "                    if mod == 't1' and 't1ce' in f.lower():\n",
        "                        continue\n",
        "                    modalities[mod] = nib.load(os.path.join(patient_path, f)).get_fdata()\n",
        "                    break\n",
        "    \n",
        "    # Normalize and stack\n",
        "    image = np.stack([\n",
        "        normalize_volume(modalities['t1'].astype(np.float32)),\n",
        "        normalize_volume(modalities['t1ce'].astype(np.float32)),\n",
        "        normalize_volume(modalities['t2'].astype(np.float32)),\n",
        "        normalize_volume(modalities['flair'].astype(np.float32))\n",
        "    ], axis=0)\n",
        "    \n",
        "    # Inference\n",
        "    image_tensor = torch.from_numpy(image).unsqueeze(0).to(DEVICE)\n",
        "    probs = simple_resize_inference(model, image_tensor)\n",
        "    prediction = torch.argmax(probs, dim=1).squeeze().cpu().numpy()\n",
        "    \n",
        "    # Find slice with most tumor\n",
        "    tumor_per_slice = [(prediction[:, :, z] > 0).sum() for z in range(prediction.shape[2])]\n",
        "    best_slice = np.argmax(tumor_per_slice)\n",
        "    \n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
        "    \n",
        "    axes[0].imshow(np.rot90(modalities['t1'][:, :, best_slice]), cmap='gray')\n",
        "    axes[0].set_title('T1')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    axes[1].imshow(np.rot90(modalities['t1ce'][:, :, best_slice]), cmap='gray')\n",
        "    axes[1].set_title('T1ce')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    axes[2].imshow(np.rot90(modalities['t2'][:, :, best_slice]), cmap='gray')\n",
        "    axes[2].set_title('T2')\n",
        "    axes[2].axis('off')\n",
        "    \n",
        "    axes[3].imshow(np.rot90(modalities['flair'][:, :, best_slice]), cmap='gray')\n",
        "    axes[3].set_title('FLAIR')\n",
        "    axes[3].axis('off')\n",
        "    \n",
        "    # Custom colormap for segmentation\n",
        "    cmap = mcolors.ListedColormap(['black', 'red', 'green', 'yellow'])\n",
        "    axes[4].imshow(np.rot90(prediction[:, :, best_slice]), cmap=cmap, vmin=0, vmax=3)\n",
        "    axes[4].set_title('Prediction')\n",
        "    axes[4].axis('off')\n",
        "    \n",
        "    plt.suptitle(f\"Patient: {patient_id} | Slice: {best_slice}\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "TEST_PATH = \"/kaggle/input/instant-odc-ai-hackathon/test\"\n",
        "# visualize_test_prediction(model, TEST_PATH)"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 6. Generate Submission\n",
        "\n",
        "Generate RLE-encoded predictions for competition submission.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def rle_encode(mask):\n",
        "    \"\"\"Encode binary mask to Run-Length Encoding.\"\"\"\n",
        "    pixels = mask.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)\n",
        "\n",
        "\n",
        "def generate_submission(model, test_path, output_path=\"submission.csv\"):\n",
        "    \"\"\"Generate submission CSV with RLE-encoded predictions.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    submission_rows = []\n",
        "    patients = sorted([p for p in os.listdir(test_path) if os.path.isdir(os.path.join(test_path, p))])\n",
        "    \n",
        "    for patient_id in tqdm(patients, desc=\"Processing\"):\n",
        "        patient_path = os.path.join(test_path, patient_id)\n",
        "        \n",
        "        # Load modalities\n",
        "        modalities = {}\n",
        "        for f in os.listdir(patient_path):\n",
        "            if f.endswith('.nii.gz'):\n",
        "                for mod in ['t1ce', 't1', 't2', 'flair']:\n",
        "                    if mod in f.lower():\n",
        "                        if mod == 't1' and 't1ce' in f.lower():\n",
        "                            continue\n",
        "                        modalities[mod] = nib.load(os.path.join(patient_path, f)).get_fdata()\n",
        "                        break\n",
        "        \n",
        "        if len(modalities) < 4:\n",
        "            continue\n",
        "        \n",
        "        # Normalize and stack\n",
        "        image = np.stack([\n",
        "            normalize_volume(modalities['t1'].astype(np.float32)),\n",
        "            normalize_volume(modalities['t1ce'].astype(np.float32)),\n",
        "            normalize_volume(modalities['t2'].astype(np.float32)),\n",
        "            normalize_volume(modalities['flair'].astype(np.float32))\n",
        "        ], axis=0)\n",
        "        \n",
        "        # Inference\n",
        "        image_tensor = torch.from_numpy(image).unsqueeze(0).to(DEVICE)\n",
        "        probs = simple_resize_inference(model, image_tensor)\n",
        "        prediction = torch.argmax(probs, dim=1).squeeze().cpu().numpy()\n",
        "        \n",
        "        # Generate RLE for each region\n",
        "        wt_mask = (prediction > 0).astype(np.uint8)\n",
        "        tc_mask = ((prediction == 1) | (prediction == 3)).astype(np.uint8)\n",
        "        et_mask = (prediction == 3).astype(np.uint8)\n",
        "        \n",
        "        submission_rows.append({'id': f'{patient_id}_WT', 'rle': rle_encode(wt_mask)})\n",
        "        submission_rows.append({'id': f'{patient_id}_TC', 'rle': rle_encode(tc_mask)})\n",
        "        submission_rows.append({'id': f'{patient_id}_ET', 'rle': rle_encode(et_mask)})\n",
        "    \n",
        "    df = pd.DataFrame(submission_rows)\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"Submission saved to {output_path}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# Generate submission\n",
        "# df = generate_submission(model, TEST_PATH)"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}