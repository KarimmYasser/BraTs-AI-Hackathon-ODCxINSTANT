{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedNeXt Inference Notebook\n",
    "This notebook runs MedNeXt inference on multiâ€‘modal brain MRI volumes, generates submission files, and provides optional visualization and verification tools.\n",
    "\n",
    "**Sections**\n",
    "1. Setup and configuration\n",
    "2. Model loading\n",
    "3. Dataset and preprocessing\n",
    "4. Inference (sliding window + optional TTA)\n",
    "5. Submission export\n",
    "6. Optional visualization and verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Clone the MedNeXt repository and install it in editable mode. Skip this section if the package is already available in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T09:39:27.889072Z",
     "iopub.status.busy": "2026-02-03T09:39:27.888313Z",
     "iopub.status.idle": "2026-02-03T09:39:47.562093Z",
     "shell.execute_reply": "2026-02-03T09:39:47.561095Z",
     "shell.execute_reply.started": "2026-02-03T09:39:27.889043Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'mednext' already exists and is not an empty directory.\n",
      "Obtaining file:///kaggle/working/mednext\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch>1.10.0 in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (2.8.0+cu126)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (4.67.1)\n",
      "Requirement already satisfied: dicom2nifti in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (2.6.2)\n",
      "Requirement already satisfied: scikit-image>=0.14 in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (0.25.2)\n",
      "Requirement already satisfied: medpy in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (0.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (1.15.3)\n",
      "Requirement already satisfied: batchgenerators>=0.23 in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (0.25.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (1.6.1)\n",
      "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (2.5.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (2.2.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (2.32.5)\n",
      "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (5.3.2)\n",
      "Requirement already satisfied: tifffile in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (2025.10.16)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mednextv1==1.7.0) (3.10.0)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from batchgenerators>=0.23->mednextv1==1.7.0) (11.3.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from batchgenerators>=0.23->mednextv1==1.7.0) (1.0.0)\n",
      "Requirement already satisfied: unittest2 in /usr/local/lib/python3.12/dist-packages (from batchgenerators>=0.23->mednextv1==1.7.0) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.12/dist-packages (from batchgenerators>=0.23->mednextv1==1.7.0) (3.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (3.5)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (2.37.0)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (26.0rc2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14->mednextv1==1.7.0) (0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>1.10.0->mednextv1==1.7.0) (3.4.0)\n",
      "Requirement already satisfied: pydicom>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dicom2nifti->mednextv1==1.7.0) (3.0.1)\n",
      "Requirement already satisfied: python-gdcm in /usr/local/lib/python3.12/dist-packages (from dicom2nifti->mednextv1==1.7.0) (3.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mednextv1==1.7.0) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mednextv1==1.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mednextv1==1.7.0) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mednextv1==1.7.0) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mednextv1==1.7.0) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mednextv1==1.7.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->mednextv1==1.7.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->mednextv1==1.7.0) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->mednextv1==1.7.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->mednextv1==1.7.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->mednextv1==1.7.0) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->mednextv1==1.7.0) (2026.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->mednextv1==1.7.0) (1.5.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mednextv1==1.7.0) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>1.10.0->mednextv1==1.7.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>1.10.0->mednextv1==1.7.0) (3.0.3)\n",
      "Collecting argparse (from unittest2->batchgenerators>=0.23->mednextv1==1.7.0)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: traceback2 in /usr/local/lib/python3.12/dist-packages (from unittest2->batchgenerators>=0.23->mednextv1==1.7.0) (1.4.0)\n",
      "Requirement already satisfied: linecache2 in /usr/local/lib/python3.12/dist-packages (from traceback2->unittest2->batchgenerators>=0.23->mednextv1==1.7.0) (1.0.0)\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: argparse, mednextv1\n",
      "  Attempting uninstall: mednextv1\n",
      "    Found existing installation: mednextv1 1.7.0\n",
      "    Uninstalling mednextv1-1.7.0:\n",
      "      Successfully uninstalled mednextv1-1.7.0\n",
      "  Running setup.py develop for mednextv1\n",
      "Successfully installed argparse-1.4.0 mednextv1-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/MIC-DKFZ/MedNeXt.git mednext\n",
    "!pip install -e ./mednext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and model loading\n",
    "Set paths, model configuration, and load the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T09:39:47.564385Z",
     "iopub.status.busy": "2026-02-03T09:39:47.564100Z",
     "iopub.status.idle": "2026-02-03T09:39:50.049335Z",
     "shell.execute_reply": "2026-02-03T09:39:50.048675Z",
     "shell.execute_reply.started": "2026-02-03T09:39:47.564358Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MedNeXt library imported successfully!\n",
      "Initializing model architecture...\n",
      "Loading weights from /kaggle/input/mednext/pytorch/default/1/best_model.pt...\n",
      "âœ… Model loaded successfully (ignored auxiliary training heads)!\n",
      "   (Best Validation Dice: 0.8268)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "repo_path = os.path.abspath(\"mednext\")\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.insert(0, repo_path)\n",
    "\n",
    "try:\n",
    "    from nnunet_mednext import create_mednext_v1\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        \"MedNeXt import failed. Ensure the repository is cloned and installed.\"\n",
    "    ) from e\n",
    "\n",
    "MODEL_PATH = \"/kaggle/input/mednext/pytorch/default/1/best_model.pt\"\n",
    "DATASET_PATH = \"/kaggle/input/instant-odc-ai-hackathon/test\"\n",
    "\n",
    "class InferenceConfig:\n",
    "    MODEL_SIZE = \"B\"\n",
    "    KERNEL_SIZE = 3\n",
    "    IN_CHANNELS = 4\n",
    "    NUM_CLASSES = 4\n",
    "    PATCH_SIZE = (128, 128, 128)\n",
    "    USE_AMP = True\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_mednext_model(checkpoint_path, config):\n",
    "    model = create_mednext_v1(\n",
    "        num_input_channels=config.IN_CHANNELS,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        model_id=config.MODEL_SIZE,\n",
    "        kernel_size=config.KERNEL_SIZE,\n",
    "        deep_supervision=False,\n",
    "    )\n",
    "    model.to(config.DEVICE)\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=config.DEVICE, weights_only=False)\n",
    "    if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
    "        state_dict = checkpoint[\"model_state_dict\"]\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "inference_model = load_mednext_model(MODEL_PATH, InferenceConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset and preprocessing\n",
    "Load multiâ€‘modal volumes and apply robust intensity normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T09:39:50.369066Z",
     "iopub.status.busy": "2026-02-03T09:39:50.368790Z",
     "iopub.status.idle": "2026-02-03T09:39:50.813881Z",
     "shell.execute_reply": "2026-02-03T09:39:50.813068Z",
     "shell.execute_reply.started": "2026-02-03T09:39:50.369044Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 334 subjects in dataset\n",
      "Sample subjects: ['BraTS2021_01333', 'BraTS2021_01334', 'BraTS2021_01335']\n",
      "Files in first subject folder: ['BraTS2021_01333_t2.nii', 'BraTS2021_01333_t1.nii', 'BraTS2021_01333_t1ce.nii', 'BraTS2021_01333_flair.nii']\n",
      "Detected modalities: ['t1', 't1ce', 't2', 'flair']\n",
      "\n",
      "Inference dataset created with 334 subjects\n"
     ]
    }
   ],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for inference that loads full volumes without patch extraction.\n",
    "    Auto-detects common modality naming patterns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str, subject_ids: List[str] = None):\n",
    "        self.data_dir = data_dir\n",
    "        self.modality_patterns = [\n",
    "            [\"t1\", \"t1ce\", \"t2\", \"flair\"],\n",
    "            [\"T1\", \"T1ce\", \"T2\", \"FLAIR\"],\n",
    "            [\"T1\", \"T1CE\", \"T2\", \"FLAIR\"],\n",
    "            [\"t1\", \"t1Gd\", \"t2\", \"flair\"],\n",
    "        ]\n",
    "        if subject_ids is None and os.path.exists(data_dir):\n",
    "            all_items = os.listdir(data_dir)\n",
    "            self.subject_ids = sorted(\n",
    "                [d for d in all_items if os.path.isdir(os.path.join(data_dir, d))]\n",
    "            )\n",
    "        else:\n",
    "            self.subject_ids = subject_ids or []\n",
    "\n",
    "        self.modalities = self._detect_modality_pattern()\n",
    "        print(f\"Subjects: {len(self.subject_ids)}\")\n",
    "        print(f\"Modalities: {self.modalities}\")\n",
    "\n",
    "    def _detect_modality_pattern(self):\n",
    "        if len(self.subject_ids) == 0:\n",
    "            return [\"t1\", \"t1ce\", \"t2\", \"flair\"]\n",
    "\n",
    "        subject_id = self.subject_ids[0]\n",
    "        subject_dir = os.path.join(self.data_dir, subject_id)\n",
    "        if not os.path.exists(subject_dir):\n",
    "            return [\"t1\", \"t1ce\", \"t2\", \"flair\"]\n",
    "\n",
    "        files = os.listdir(subject_dir)\n",
    "        for pattern in self.modality_patterns:\n",
    "            found_all = True\n",
    "            for mod in pattern:\n",
    "                possible_names = [\n",
    "                    f\"{subject_id}_{mod}.nii.gz\",\n",
    "                    f\"{subject_id}-{mod}.nii.gz\",\n",
    "                    f\"{subject_id}_{mod}.nii\",\n",
    "                    f\"{mod}.nii.gz\",\n",
    "                    f\"{mod}.nii\",\n",
    "                ]\n",
    "                if not any(name in files for name in possible_names):\n",
    "                    found_all = False\n",
    "                    break\n",
    "            if found_all:\n",
    "                return pattern\n",
    "\n",
    "        detected = []\n",
    "        modality_keywords = {\n",
    "            \"t1ce\": [\"t1ce\", \"t1gd\", \"t1Gd\", \"T1CE\", \"T1Gd\", \"T1ce\"],\n",
    "            \"t1\": [\"t1\", \"T1\"],\n",
    "            \"t2\": [\"t2\", \"T2\"],\n",
    "            \"flair\": [\"flair\", \"FLAIR\", \"Flair\"],\n",
    "        }\n",
    "        for mod_key, keywords in modality_keywords.items():\n",
    "            for f in files:\n",
    "                f_lower = f.lower()\n",
    "                if any(kw.lower() in f_lower for kw in keywords):\n",
    "                    for kw in keywords:\n",
    "                        if kw in f:\n",
    "                            detected.append(kw)\n",
    "                            break\n",
    "                    break\n",
    "        if len(detected) == 4:\n",
    "            return detected\n",
    "        return [\"t1\", \"t1ce\", \"t2\", \"flair\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subject_ids)\n",
    "\n",
    "    def _find_modality_file(self, subject_dir: str, subject_id: str, modality: str) -> str:\n",
    "        files = os.listdir(subject_dir)\n",
    "        patterns = [\n",
    "            f\"{subject_id}_{modality}.nii.gz\",\n",
    "            f\"{subject_id}-{modality}.nii.gz\",\n",
    "            f\"{subject_id}_{modality}.nii\",\n",
    "            f\"{modality}.nii.gz\",\n",
    "            f\"{modality}.nii\",\n",
    "        ]\n",
    "        for pattern in patterns:\n",
    "            if pattern in files:\n",
    "                return os.path.join(subject_dir, pattern)\n",
    "        modality_lower = modality.lower()\n",
    "        for f in files:\n",
    "            if modality_lower in f.lower() and (\"nii\" in f.lower()):\n",
    "                if modality_lower == \"t1\":\n",
    "                    if \"t1ce\" not in f.lower() and \"t1gd\" not in f.lower():\n",
    "                        return os.path.join(subject_dir, f)\n",
    "                else:\n",
    "                    return os.path.join(subject_dir, f)\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not find {modality} in {subject_dir}. Files: {files}\"\n",
    "        )\n",
    "\n",
    "    def _load_nifti(self, filepath: str) -> np.ndarray:\n",
    "        img = nib.load(filepath)\n",
    "        return img.get_fdata().astype(np.float32)\n",
    "\n",
    "    def _normalize(self, data: np.ndarray) -> np.ndarray:\n",
    "        mask = data > 0\n",
    "        if mask.sum() == 0:\n",
    "            return data\n",
    "        pixels = data[mask]\n",
    "        p_low, p_high = np.percentile(pixels, 0.5), np.percentile(pixels, 99.5)\n",
    "        data = np.clip(data, p_low, p_high)\n",
    "        pixels = data[mask]\n",
    "        mean, std = pixels.mean(), pixels.std()\n",
    "        data = (data - mean) / (std + 1e-8)\n",
    "        data[~mask] = 0\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        subject_id = self.subject_ids[idx]\n",
    "        subject_dir = os.path.join(self.data_dir, subject_id)\n",
    "        modality_data = []\n",
    "        for mod in self.modalities:\n",
    "            filepath = self._find_modality_file(subject_dir, subject_id, mod)\n",
    "            data = self._load_nifti(filepath)\n",
    "            data = self._normalize(data)\n",
    "            modality_data.append(data)\n",
    "        volume = np.stack(modality_data, axis=0)\n",
    "\n",
    "        seg = None\n",
    "        seg_patterns = [\n",
    "            f\"{subject_id}_seg.nii.gz\",\n",
    "            f\"{subject_id}-seg.nii.gz\",\n",
    "            \"seg.nii.gz\",\n",
    "            f\"{subject_id}_mask.nii.gz\",\n",
    "        ]\n",
    "        files = os.listdir(subject_dir)\n",
    "        for pattern in seg_patterns:\n",
    "            if pattern in files:\n",
    "                seg_path = os.path.join(subject_dir, pattern)\n",
    "                seg = self._load_nifti(seg_path)\n",
    "                new_seg = np.zeros_like(seg)\n",
    "                new_seg[seg == 0] = 0\n",
    "                new_seg[seg == 1] = 1\n",
    "                new_seg[seg == 2] = 2\n",
    "                new_seg[seg == 4] = 3\n",
    "                seg = new_seg\n",
    "                break\n",
    "\n",
    "        volume_tensor = torch.from_numpy(volume.copy()).float()\n",
    "        return {\n",
    "            \"subject_id\": subject_id,\n",
    "            \"volume\": volume_tensor,\n",
    "            \"ground_truth\": seg,\n",
    "            \"original_shape\": volume.shape[1:],\n",
    "        }\n",
    "\n",
    "inference_dataset = InferenceDataset(DATASET_PATH)\n",
    "print(f\"Inference dataset size: {len(inference_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference utilities\n",
    "Sliding-window inference with optional test-time augmentation (TTA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T09:39:50.815056Z",
     "iopub.status.busy": "2026-02-03T09:39:50.814823Z",
     "iopub.status.idle": "2026-02-03T09:39:50.827079Z",
     "shell.execute_reply": "2026-02-03T09:39:50.826461Z",
     "shell.execute_reply.started": "2026-02-03T09:39:50.815034Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliding window inference function defined!\n"
     ]
    }
   ],
   "source": [
    "def setup_multi_gpu(model):\n",
    "    \"\"\"Wrap model with DataParallel for multi-GPU inference.\"\"\"\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    return model\n",
    "\n",
    "inference_model = setup_multi_gpu(inference_model)\n",
    "\n",
    "def rle_encode_c_order(mask):\n",
    "    \"\"\"Encode a binary mask using C-order flattening.\"\"\"\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return \" \".join(str(x) for x in runs)\n",
    "\n",
    "def sliding_window_inference_logits(model, volume, config, overlap=0.6):\n",
    "    \"\"\"Return softmax probabilities for TTA accumulation.\"\"\"\n",
    "    patch_size = config.PATCH_SIZE\n",
    "    device = config.DEVICE\n",
    "    pd, ph, pw = patch_size\n",
    "    C, D, H, W = volume.shape\n",
    "    stride_d = int(pd * (1 - overlap))\n",
    "    stride_h = int(ph * (1 - overlap))\n",
    "    stride_w = int(pw * (1 - overlap))\n",
    "    pad_d = max(0, pd - D)\n",
    "    pad_h = max(0, ph - H)\n",
    "    pad_w = max(0, pw - W)\n",
    "    if pad_d > 0 or pad_h > 0 or pad_w > 0:\n",
    "        volume = F.pad(volume, (0, pad_w, 0, pad_h, 0, pad_d))\n",
    "        D, H, W = volume.shape[1:]\n",
    "    output = torch.zeros((config.NUM_CLASSES, D, H, W), device=device)\n",
    "    count = torch.zeros((D, H, W), device=device)\n",
    "    d_pos = list(range(0, max(1, D - pd + 1), stride_d))\n",
    "    h_pos = list(range(0, max(1, H - ph + 1), stride_h))\n",
    "    w_pos = list(range(0, max(1, W - pw + 1), stride_w))\n",
    "    if D > pd and D - pd not in d_pos:\n",
    "        d_pos.append(D - pd)\n",
    "    if H > ph and H - ph not in h_pos:\n",
    "        h_pos.append(H - ph)\n",
    "    if W > pw and W - pw not in w_pos:\n",
    "        w_pos.append(W - pw)\n",
    "    all_patches = []\n",
    "    all_coords = []\n",
    "    for d in d_pos:\n",
    "        for h in h_pos:\n",
    "            for w in w_pos:\n",
    "                patch = volume[:, d : d + pd, h : h + ph, w : w + pw]\n",
    "                all_patches.append(patch)\n",
    "                all_coords.append((d, h, w))\n",
    "    batch_size = max(1, torch.cuda.device_count() * 2)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(all_patches), batch_size):\n",
    "            batch_patches = all_patches[i : i + batch_size]\n",
    "            batch_coords = all_coords[i : i + batch_size]\n",
    "            batch_tensor = torch.stack(batch_patches, dim=0).to(device)\n",
    "            with torch.amp.autocast(\"cuda\", enabled=config.USE_AMP):\n",
    "                preds = model(batch_tensor)\n",
    "            if isinstance(preds, (list, tuple)):\n",
    "                preds = preds[0]\n",
    "            preds = F.softmax(preds, dim=1)\n",
    "            for j, (d, h, w) in enumerate(batch_coords):\n",
    "                output[:, d : d + pd, h : h + ph, w : w + pw] += preds[j]\n",
    "                count[d : d + pd, h : h + ph, w : w + pw] += 1\n",
    "    output /= count.unsqueeze(0).clamp(min=1)\n",
    "    original_d = D - pad_d if pad_d > 0 else D\n",
    "    original_h = H - pad_h if pad_h > 0 else H\n",
    "    original_w = W - pad_w if pad_w > 0 else W\n",
    "    return output[:, :original_d, :original_h, :original_w]\n",
    "\n",
    "def predict_with_tta(model, volume, config, overlap=0.6, use_tta=True):\n",
    "    \"\"\"Run sliding-window inference with optional TTA.\"\"\"\n",
    "    model.eval()\n",
    "    logits_sum = sliding_window_inference_logits(model, volume, config, overlap=overlap)\n",
    "    num_predictions = 1\n",
    "    if use_tta:\n",
    "        flip_axes = [[0], [1], [2]]\n",
    "        for axes in flip_axes:\n",
    "            flip_dim = [a + 1 for a in axes]\n",
    "            flipped_vol = torch.flip(volume, dims=flip_dim)\n",
    "            flipped_logits = sliding_window_inference_logits(\n",
    "                model, flipped_vol, config, overlap=overlap\n",
    "            )\n",
    "            unflipped_logits = torch.flip(flipped_logits, dims=flip_dim)\n",
    "            logits_sum += unflipped_logits\n",
    "            num_predictions += 1\n",
    "    avg_logits = logits_sum / num_predictions\n",
    "    final_pred = torch.argmax(avg_logits, dim=0).cpu().numpy()\n",
    "    return final_pred\n",
    "\n",
    "def sliding_window_inference(model, volume, config, overlap=0.5):\n",
    "    \"\"\"Wrapper without TTA.\"\"\"\n",
    "    return predict_with_tta(model, volume, config, overlap=overlap, use_tta=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run inference and export submission\n",
    "Generate run-length encoding (RLE) per class and save to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T09:39:50.828113Z",
     "iopub.status.busy": "2026-02-03T09:39:50.827849Z",
     "iopub.status.idle": "2026-02-03T10:17:38.286638Z",
     "shell.execute_reply": "2026-02-03T10:17:38.285894Z",
     "shell.execute_reply.started": "2026-02-03T09:39:50.828083Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference on 334 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/334 [00:00<?, ?it/s]/tmp/ipykernel_55/4204801297.py:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=config.USE_AMP):\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [37:46<00:00,  6.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Submission generated with 1002 rows.\n",
      "                  id                                                rle\n",
      "0  BraTS2021_01333_1  3668819 1 3854669 1 3891714 1 3891866 4 389201...\n",
      "1  BraTS2021_01333_2  2399957 5 2400111 5 2400117 3 2400266 13 24004...\n",
      "2  BraTS2021_01333_4  3668202 2 3668355 4 3668510 4 3668665 3 370524...\n",
      "3  BraTS2021_01334_1  2250059 2 2287104 1 2287259 1 2287414 1 228756...\n",
      "4  BraTS2021_01334_2  2064365 3 2064521 3 2064676 3 2064831 4 206498...\n"
     ]
    }
   ],
   "source": [
    "USE_TTA = True\n",
    "OVERLAP = 0.6\n",
    "CLEANUP_EVERY = 3\n",
    "\n",
    "label_map = {1: 1, 2: 2, 3: 4}\n",
    "submission_rows = []\n",
    "\n",
    "inference_model.eval()\n",
    "for idx in tqdm(range(len(inference_dataset)), desc=\"Inference\"):\n",
    "    sample = inference_dataset[idx]\n",
    "    subject_id = sample[\"subject_id\"]\n",
    "    volume = sample[\"volume\"]\n",
    "    prediction = predict_with_tta(\n",
    "        inference_model,\n",
    "        volume,\n",
    "        InferenceConfig,\n",
    "        overlap=OVERLAP,\n",
    "        use_tta=USE_TTA,\n",
    "    )\n",
    "    for model_label, comp_label in label_map.items():\n",
    "        binary_mask = (prediction == model_label).astype(np.uint8)\n",
    "        rle_string = rle_encode_c_order(binary_mask) if binary_mask.sum() > 0 else \"\"\n",
    "        submission_rows.append({\"id\": f\"{subject_id}_{comp_label}\", \"rle\": rle_string})\n",
    "    if idx % CLEANUP_EVERY == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "submission_df = pd.DataFrame(submission_rows)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(f\"Saved submission.csv with {len(submission_df)} rows\")\n",
    "submission_df.head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optional: interactive visualization\n",
    "Use this section to inspect predictions alongside MRI slices and (optionally) ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T10:35:50.952666Z",
     "iopub.status.busy": "2026-02-03T10:35:50.951763Z",
     "iopub.status.idle": "2026-02-03T10:35:58.137771Z",
     "shell.execute_reply": "2026-02-03T10:35:58.137147Z",
     "shell.execute_reply.started": "2026-02-03T10:35:50.952636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 917 subjects in nested dataset\n",
      "Sample subjects: ['BraTS2021_00000', 'BraTS2021_00002', 'BraTS2021_00003']\n",
      "Subfolders in first patient: ['BraTS2021_00000_seg.nii', 'BraTS2021_00000_flair.nii', 'BraTS2021_00000_t1ce.nii', 'BraTS2021_00000_t2.nii', 'BraTS2021_00000_t1.nii']\n",
      "Files in 'BraTS2021_00000_seg.nii': ['00000057_final_seg.nii']\n",
      "âœ… Training dataset loaded with 917 subjects (with ground truth)\n",
      "ðŸ“‹ Total training subjects: 917\n",
      "First 10 subjects:\n",
      "   0: BraTS2021_00000\n",
      "   1: BraTS2021_00002\n",
      "   2: BraTS2021_00003\n",
      "   3: BraTS2021_00005\n",
      "   4: BraTS2021_00006\n",
      "   5: BraTS2021_00008\n",
      "   6: BraTS2021_00009\n",
      "   7: BraTS2021_00011\n",
      "   8: BraTS2021_00012\n",
      "   9: BraTS2021_00014\n",
      "ðŸ” Loading patient: BraTS2021_01333\n",
      "   Ground truth available: âŒ No\n",
      "ðŸ§  Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/4204801297.py:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=config.USE_AMP):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inference complete! Volume shape: (240, 240, 155)\n",
      "   Predicted classes: [0 1 2 3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02f332cff734dc89ad56c6c21f410d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=123, description='Axial (Z):', max=154), IntSlider(value=102, descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Tumor Statistics (Prediction):\n",
      "   NCR/NET: 43 voxels\n",
      "   Edema: 3,901 voxels\n",
      "   Enhancing: 1,099 voxels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "class NestedFolderDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for nested folder structures with one subfolder per modality.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = data_dir\n",
    "        self.subject_ids = []\n",
    "        if os.path.exists(data_dir):\n",
    "            all_items = os.listdir(data_dir)\n",
    "            self.subject_ids = sorted(\n",
    "                [d for d in all_items if os.path.isdir(os.path.join(data_dir, d))]\n",
    "            )\n",
    "        print(f\"Nested dataset subjects: {len(self.subject_ids)}\")\n",
    "        if self.subject_ids:\n",
    "            print(f\"Sample subjects: {self.subject_ids[:3]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subject_ids)\n",
    "\n",
    "    def _find_nifti_in_folder(self, folder_path):\n",
    "        if not os.path.isdir(folder_path):\n",
    "            return None\n",
    "        for f in os.listdir(folder_path):\n",
    "            if f.endswith(\".nii.gz\"):\n",
    "                return os.path.join(folder_path, f)\n",
    "        for f in os.listdir(folder_path):\n",
    "            if f.endswith(\".nii\"):\n",
    "                return os.path.join(folder_path, f)\n",
    "        return None\n",
    "\n",
    "    def _find_modality_file(self, patient_path, subject_id, modality):\n",
    "        items = os.listdir(patient_path)\n",
    "        modality_lower = modality.lower()\n",
    "        for item in items:\n",
    "            item_path = os.path.join(patient_path, item)\n",
    "            item_lower = item.lower()\n",
    "            if os.path.isdir(item_path):\n",
    "                if modality_lower == \"t1\":\n",
    "                    if (\"t1\" in item_lower and \"t1ce\" not in item_lower and \"t1gd\" not in item_lower):\n",
    "                        nifti_file = self._find_nifti_in_folder(item_path)\n",
    "                        if nifti_file:\n",
    "                            return nifti_file\n",
    "                elif modality_lower in item_lower:\n",
    "                    nifti_file = self._find_nifti_in_folder(item_path)\n",
    "                    if nifti_file:\n",
    "                        return nifti_file\n",
    "        direct_patterns = [\n",
    "            os.path.join(patient_path, f\"{subject_id}_{modality}.nii.gz\"),\n",
    "            os.path.join(patient_path, f\"{subject_id}_{modality}.nii\"),\n",
    "            os.path.join(patient_path, f\"{modality}.nii.gz\"),\n",
    "            os.path.join(patient_path, f\"{modality}.nii\"),\n",
    "        ]\n",
    "        for pattern in direct_patterns:\n",
    "            if os.path.exists(pattern):\n",
    "                return pattern\n",
    "        raise FileNotFoundError(f\"Could not find {modality} for {subject_id}\")\n",
    "\n",
    "    def _load_nifti(self, filepath):\n",
    "        img = nib.load(filepath)\n",
    "        return img.get_fdata().astype(np.float32)\n",
    "\n",
    "    def _normalize(self, data):\n",
    "        mask = data > 0\n",
    "        if mask.sum() == 0:\n",
    "            return data\n",
    "        pixels = data[mask]\n",
    "        p_low, p_high = np.percentile(pixels, 0.5), np.percentile(pixels, 99.5)\n",
    "        data = np.clip(data, p_low, p_high)\n",
    "        pixels = data[mask]\n",
    "        mean, std = pixels.mean(), pixels.std()\n",
    "        data = (data - mean) / (std + 1e-8)\n",
    "        data[~mask] = 0\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.subject_ids[idx]\n",
    "        patient_path = os.path.join(self.data_dir, subject_id)\n",
    "        modalities = [\"t1\", \"t1ce\", \"t2\", \"flair\"]\n",
    "        modality_data = []\n",
    "        for mod in modalities:\n",
    "            try:\n",
    "                filepath = self._find_modality_file(patient_path, subject_id, mod)\n",
    "                data = self._load_nifti(filepath)\n",
    "                data = self._normalize(data)\n",
    "                modality_data.append(data)\n",
    "            except Exception:\n",
    "                if modality_data:\n",
    "                    modality_data.append(np.zeros_like(modality_data[0]))\n",
    "                else:\n",
    "                    raise\n",
    "        volume = np.stack(modality_data, axis=0)\n",
    "        seg = None\n",
    "        try:\n",
    "            seg_file = self._find_modality_file(patient_path, subject_id, \"seg\")\n",
    "            seg = self._load_nifti(seg_file)\n",
    "            new_seg = np.zeros_like(seg)\n",
    "            new_seg[seg == 1] = 1\n",
    "            new_seg[seg == 2] = 2\n",
    "            new_seg[seg == 4] = 3\n",
    "            seg = new_seg\n",
    "        except Exception:\n",
    "            pass\n",
    "        volume_tensor = torch.from_numpy(volume.copy()).float()\n",
    "        return {\n",
    "            \"subject_id\": subject_id,\n",
    "            \"volume\": volume_tensor,\n",
    "            \"ground_truth\": seg,\n",
    "            \"original_shape\": volume.shape[1:],\n",
    "        }\n",
    "\n",
    "TRAIN_PATH = \"/kaggle/input/instant-odc-ai-hackathon/Train\"\n",
    "train_dataset = NestedFolderDataset(TRAIN_PATH)\n",
    "\n",
    "def visualize_3d_by_id(subject_id, model=None, dataset=None, use_train=True):\n",
    "    \"\"\"Interactive 3D visualization for a specific patient.\"\"\"\n",
    "    if model is None:\n",
    "        model = inference_model\n",
    "    if dataset is None:\n",
    "        dataset = train_dataset if use_train else inference_dataset\n",
    "    try:\n",
    "        patient_idx = dataset.subject_ids.index(subject_id)\n",
    "    except ValueError:\n",
    "        print(f\"Subject not found: {subject_id}\")\n",
    "        print(f\"Available subjects (first 10): {dataset.subject_ids[:10]}\")\n",
    "        return\n",
    "    sample = dataset[patient_idx]\n",
    "    volume = sample[\"volume\"]\n",
    "    ground_truth = sample[\"ground_truth\"]\n",
    "    has_gt = ground_truth is not None\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = sliding_window_inference(\n",
    "            model, volume, InferenceConfig, overlap=0.5\n",
    "        )\n",
    "    volume_np = volume.numpy()\n",
    "    D, H, W = prediction.shape\n",
    "    cmap = mcolors.ListedColormap([\"none\", \"red\", \"green\", \"yellow\"])\n",
    "    bounds = [0, 0.5, 1.5, 2.5, 3.5]\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    def show_slices(axial_slice, sagittal_slice, coronal_slice):\n",
    "        n_rows = 3 if has_gt else 2\n",
    "        fig, axes = plt.subplots(n_rows, 3, figsize=(18, 6 * n_rows))\n",
    "        flair_ax = volume_np[3, :, :, axial_slice]\n",
    "        flair_sag = volume_np[3, sagittal_slice, :, :]\n",
    "        flair_cor = volume_np[3, :, coronal_slice, :]\n",
    "        pred_ax = prediction[:, :, axial_slice]\n",
    "        pred_sag = prediction[sagittal_slice, :, :]\n",
    "        pred_cor = prediction[:, coronal_slice, :]\n",
    "        axes[0, 0].imshow(flair_ax, cmap=\"gray\")\n",
    "        axes[0, 0].set_title(f\"Axial (Z={axial_slice})\")\n",
    "        axes[0, 0].axis(\"off\")\n",
    "        axes[0, 1].imshow(flair_sag.T, cmap=\"gray\", origin=\"lower\")\n",
    "        axes[0, 1].set_title(f\"Sagittal (X={sagittal_slice})\")\n",
    "        axes[0, 1].axis(\"off\")\n",
    "        axes[0, 2].imshow(flair_cor.T, cmap=\"gray\", origin=\"lower\")\n",
    "        axes[0, 2].set_title(f\"Coronal (Y={coronal_slice})\")\n",
    "        axes[0, 2].axis(\"off\")\n",
    "        axes[1, 0].imshow(flair_ax, cmap=\"gray\", alpha=0.7)\n",
    "        axes[1, 0].imshow(pred_ax, cmap=cmap, norm=norm, alpha=0.6)\n",
    "        axes[1, 0].set_title(\"Axial - Prediction\")\n",
    "        axes[1, 0].axis(\"off\")\n",
    "        axes[1, 1].imshow(flair_sag.T, cmap=\"gray\", origin=\"lower\", alpha=0.7)\n",
    "        axes[1, 1].imshow(pred_sag.T, cmap=cmap, norm=norm, origin=\"lower\", alpha=0.6)\n",
    "        axes[1, 1].set_title(\"Sagittal - Prediction\")\n",
    "        axes[1, 1].axis(\"off\")\n",
    "        axes[1, 2].imshow(flair_cor.T, cmap=\"gray\", origin=\"lower\", alpha=0.7)\n",
    "        axes[1, 2].imshow(pred_cor.T, cmap=cmap, norm=norm, alpha=0.6)\n",
    "        axes[1, 2].set_title(\"Coronal - Prediction\")\n",
    "        axes[1, 2].axis(\"off\")\n",
    "        if has_gt:\n",
    "            gt_ax = ground_truth[:, :, axial_slice].astype(int)\n",
    "            gt_sag = ground_truth[sagittal_slice, :, :].astype(int)\n",
    "            gt_cor = ground_truth[:, coronal_slice, :].astype(int)\n",
    "            axes[2, 0].imshow(flair_ax, cmap=\"gray\", alpha=0.7)\n",
    "            axes[2, 0].imshow(gt_ax, cmap=cmap, norm=norm, alpha=0.6)\n",
    "            axes[2, 0].set_title(\"Axial - Ground Truth\")\n",
    "            axes[2, 0].axis(\"off\")\n",
    "            axes[2, 1].imshow(flair_sag.T, cmap=\"gray\", origin=\"lower\", alpha=0.7)\n",
    "            axes[2, 1].imshow(gt_sag.T, cmap=cmap, norm=norm, origin=\"lower\", alpha=0.6)\n",
    "            axes[2, 1].set_title(\"Sagittal - Ground Truth\")\n",
    "            axes[2, 1].axis(\"off\")\n",
    "            axes[2, 2].imshow(flair_cor.T, cmap=\"gray\", origin=\"lower\", alpha=0.7)\n",
    "            axes[2, 2].imshow(gt_cor.T, cmap=cmap, norm=norm, alpha=0.6)\n",
    "            axes[2, 2].set_title(\"Coronal - Ground Truth\")\n",
    "            axes[2, 2].axis(\"off\")\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor=\"red\", label=\"NCR/NET (1)\"),\n",
    "            Patch(facecolor=\"green\", label=\"Edema (2)\"),\n",
    "            Patch(facecolor=\"yellow\", label=\"Enhancing (4)\"),\n",
    "        ]\n",
    "        fig.legend(handles=legend_elements, loc=\"lower center\", ncol=3, fontsize=11)\n",
    "        plt.suptitle(f\"Patient: {subject_id}\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.05)\n",
    "        plt.show()\n",
    "\n",
    "    tumor_per_axial = (prediction > 0).sum(axis=(0, 1))\n",
    "    tumor_per_sagittal = (prediction > 0).sum(axis=(1, 2))\n",
    "    tumor_per_coronal = (prediction > 0).sum(axis=(0, 2))\n",
    "    default_ax = int(np.argmax(tumor_per_axial)) if tumor_per_axial.max() > 0 else W // 2\n",
    "    default_sag = int(np.argmax(tumor_per_sagittal)) if tumor_per_sagittal.max() > 0 else D // 2\n",
    "    default_cor = int(np.argmax(tumor_per_coronal)) if tumor_per_coronal.max() > 0 else H // 2\n",
    "    interact(\n",
    "        show_slices,\n",
    "        axial_slice=IntSlider(min=0, max=W - 1, step=1, value=default_ax, description=\"Axial (Z):\"),\n",
    "        sagittal_slice=IntSlider(min=0, max=D - 1, step=1, value=default_sag, description=\"Sagittal (X):\"),\n",
    "        coronal_slice=IntSlider(min=0, max=H - 1, step=1, value=default_cor, description=\"Coronal (Y):\"),\n",
    "    )\n",
    "    return prediction\n",
    "\n",
    "def list_train_subjects(n=20):\n",
    "    print(f\"Training subjects: {len(train_dataset.subject_ids)}\")\n",
    "    for i, sid in enumerate(train_dataset.subject_ids[:n]):\n",
    "        print(f\"{i}: {sid}\")\n",
    "\n",
    "list_train_subjects(10)\n",
    "# visualize_3d_by_id(\"BraTS2021_00005\", use_train=True)\n",
    "# visualize_3d_by_id(\"BraTS2021_01333\", use_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optional: RLE verification\n",
    "Compare model predictions with decoded RLE from the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T10:43:35.295405Z",
     "iopub.status.busy": "2026-02-03T10:43:35.294830Z",
     "iopub.status.idle": "2026-02-03T10:43:44.218624Z",
     "shell.execute_reply": "2026-02-03T10:43:44.218048Z",
     "shell.execute_reply.started": "2026-02-03T10:43:35.295376Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded submission.csv with 1002 rows\n",
      "                  id                                                rle\n",
      "0  BraTS2021_01333_1  3668819 1 3854669 1 3891714 1 3891866 4 389201...\n",
      "1  BraTS2021_01333_2  2399957 5 2400111 5 2400117 3 2400266 13 24004...\n",
      "2  BraTS2021_01333_4  3668202 2 3668355 4 3668510 4 3668665 3 370524...\n",
      "3  BraTS2021_01334_1  2250059 2 2287104 1 2287259 1 2287414 1 228756...\n",
      "4  BraTS2021_01334_2  2064365 3 2064521 3 2064676 3 2064831 4 206498...\n",
      "5  BraTS2021_01334_4  2174885 1 2175508 2 2175663 3 2175818 3 221162...\n",
      "6  BraTS2021_01335_1  5788252 2 5788406 3 5788561 3 5788717 2 578964...\n",
      "7  BraTS2021_01335_2  4818423 1 4818577 2 4818731 3 4818885 4 481904...\n",
      "8  BraTS2021_01335_4  5305733 2 5342933 1 5379978 1 5380133 1 541717...\n",
      "9  BraTS2021_01336_1  5113240 1 5113394 2 5113538 1 5113548 3 511370...\n",
      "\n",
      "============================================================\n",
      "RLE ENCODING VERIFICATION\n",
      "============================================================\n",
      "ðŸ“‹ Total test subjects: 334\n",
      "First 5 subjects:\n",
      "   0: BraTS2021_01333\n",
      "   1: BraTS2021_01334\n",
      "   2: BraTS2021_01335\n",
      "   3: BraTS2021_01336\n",
      "   4: BraTS2021_01337\n",
      "ðŸ” Verifying encoding for: BraTS2021_01334\n",
      "   Volume shape: torch.Size([4, 240, 240, 155])\n",
      "   Original shape: (240, 240, 155)\n",
      "ðŸ§  Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/4204801297.py:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=config.USE_AMP):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Prediction shape: (240, 240, 155)\n",
      "   BraTS2021_01334_1: Decoded 1,403 voxels\n",
      "   BraTS2021_01334_2: Decoded 48,530 voxels\n",
      "   BraTS2021_01334_4: Decoded 12,179 voxels\n",
      "\n",
      "ðŸ“Š COMPARISON:\n",
      "   Class             Model Pred  Decoded CSV    Match\n",
      "   --------------------------------------------------\n",
      "   NCR/NET                1,403        1,403        âœ…\n",
      "   Edema                 48,530       48,530        âœ…\n",
      "   Enhancing             12,179       12,179        âœ…\n",
      "\n",
      "   Exact pixel-by-pixel match: âŒ NO\n",
      "   Differing voxels: 8,938 / 8,928,000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c01988d33f4ba4a43f5a57549f90eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=93, description='Slice:', max=154), Output()), _dom_classes=('widget-intâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]]),\n",
       " array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       " True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "SUBMISSION_PATH = \"/kaggle/working/submission.csv\"\n",
    "submission_df = pd.read_csv(SUBMISSION_PATH)\n",
    "print(f\"Loaded submission.csv with {len(submission_df)} rows\")\n",
    "submission_df.head(10)\n",
    "\n",
    "def rle_decode(rle_string, shape):\n",
    "    \"\"\"Decode RLE string back to a binary mask (C-order).\"\"\"\n",
    "    if pd.isna(rle_string) or rle_string == \"\" or rle_string.strip() == \"\":\n",
    "        return np.zeros(shape, dtype=np.uint8)\n",
    "    s = rle_string.split()\n",
    "    starts = np.array(s[0::2], dtype=int)\n",
    "    lengths = np.array(s[1::2], dtype=int)\n",
    "    total_size = np.prod(shape)\n",
    "    mask_1d = np.zeros(total_size, dtype=np.uint8)\n",
    "    for start, length in zip(starts, lengths):\n",
    "        mask_1d[start : start + length] = 1\n",
    "    mask = mask_1d.reshape(shape)\n",
    "    return mask\n",
    "\n",
    "def verify_encoding_by_id(subject_id, model=None, dataset=None, slice_idx=None):\n",
    "    \"\"\"Compare model prediction with decoded RLE for a subject.\"\"\"\n",
    "    if model is None:\n",
    "        model = inference_model\n",
    "    if dataset is None:\n",
    "        dataset = inference_dataset\n",
    "    try:\n",
    "        patient_idx = dataset.subject_ids.index(subject_id)\n",
    "    except ValueError:\n",
    "        print(f\"Subject not found: {subject_id}\")\n",
    "        print(f\"Available subjects (first 10): {dataset.subject_ids[:10]}\")\n",
    "        return\n",
    "    sample = dataset[patient_idx]\n",
    "    volume = sample[\"volume\"]\n",
    "    with torch.no_grad():\n",
    "        prediction = sliding_window_inference(\n",
    "            model, volume, InferenceConfig, overlap=0.5\n",
    "        )\n",
    "    label_map = {1: 1, 2: 2, 4: 3}\n",
    "    decoded_masks = {}\n",
    "    for comp_label, model_label in label_map.items():\n",
    "        row_id = f\"{subject_id}_{comp_label}\"\n",
    "        row = submission_df[submission_df[\"id\"] == row_id]\n",
    "        if len(row) == 0:\n",
    "            decoded_masks[model_label] = np.zeros(prediction.shape, dtype=np.uint8)\n",
    "        else:\n",
    "            rle_string = row[\"rle\"].values[0]\n",
    "            decoded_masks[model_label] = rle_decode(rle_string, prediction.shape)\n",
    "    decoded_combined = np.zeros(prediction.shape, dtype=np.uint8)\n",
    "    for model_label, mask in decoded_masks.items():\n",
    "        decoded_combined[mask == 1] = model_label\n",
    "    exact_match = np.array_equal(prediction, decoded_combined)\n",
    "    print(f\"Exact match: {exact_match}\")\n",
    "    if not exact_match:\n",
    "        diff = (prediction != decoded_combined)\n",
    "        print(f\"Differing voxels: {diff.sum()} / {prediction.size}\")\n",
    "    if slice_idx is None:\n",
    "        tumor_per_slice = (prediction > 0).sum(axis=(0, 1))\n",
    "        slice_idx = int(np.argmax(tumor_per_slice)) if tumor_per_slice.max() > 0 else prediction.shape[2] // 2\n",
    "    volume_np = volume.numpy()\n",
    "    flair = volume_np[3]\n",
    "    cmap = mcolors.ListedColormap([\"none\", \"red\", \"green\", \"yellow\"])\n",
    "    bounds = [0, 0.5, 1.5, 2.5, 3.5]\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    def show_comparison(slice_idx):\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        flair_slice = flair[:, :, slice_idx]\n",
    "        pred_slice = prediction[:, :, slice_idx]\n",
    "        decoded_slice = decoded_combined[:, :, slice_idx]\n",
    "        axes[0, 0].imshow(flair_slice, cmap=\"gray\")\n",
    "        axes[0, 0].set_title(f\"FLAIR (slice {slice_idx})\")\n",
    "        axes[0, 0].axis(\"off\")\n",
    "        axes[0, 1].imshow(flair_slice, cmap=\"gray\", alpha=0.7)\n",
    "        axes[0, 1].imshow(pred_slice, cmap=cmap, norm=norm, alpha=0.6)\n",
    "        axes[0, 1].set_title(\"Model prediction\")\n",
    "        axes[0, 1].axis(\"off\")\n",
    "        axes[0, 2].imshow(flair_slice, cmap=\"gray\", alpha=0.7)\n",
    "        axes[0, 2].imshow(decoded_slice, cmap=cmap, norm=norm, alpha=0.6)\n",
    "        axes[0, 2].set_title(\"Decoded from CSV\")\n",
    "        axes[0, 2].axis(\"off\")\n",
    "        for i, (model_label, name) in enumerate([(1, \"NCR/NET\"), (2, \"Edema\"), (3, \"Enhancing\")]):\n",
    "            pred_class = (prediction[:, :, slice_idx] == model_label).astype(int)\n",
    "            decoded_class = (decoded_combined[:, :, slice_idx] == model_label).astype(int)\n",
    "            diff_img = np.zeros((*pred_class.shape, 3))\n",
    "            diff_img[pred_class & decoded_class] = [0, 1, 0]\n",
    "            diff_img[pred_class & ~decoded_class] = [1, 0, 0]\n",
    "            diff_img[~pred_class & decoded_class] = [0, 0, 1]\n",
    "            axes[1, i].imshow(flair_slice, cmap=\"gray\", alpha=0.5)\n",
    "            axes[1, i].imshow(diff_img, alpha=0.7)\n",
    "            match_pct = 100 * (pred_class == decoded_class).sum() / pred_class.size\n",
    "            axes[1, i].set_title(f\"{name}\\n{match_pct:.2f}% match\")\n",
    "            axes[1, i].axis(\"off\")\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor=\"red\", label=\"NCR/NET (1)\"),\n",
    "            Patch(facecolor=\"green\", label=\"Edema (2)\"),\n",
    "            Patch(facecolor=\"yellow\", label=\"Enhancing (4)\"),\n",
    "        ]\n",
    "        fig.legend(handles=legend_elements, loc=\"lower center\", ncol=3, fontsize=11)\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.08)\n",
    "        plt.show()\n",
    "\n",
    "    interact(\n",
    "        show_comparison,\n",
    "        slice_idx=IntSlider(min=0, max=prediction.shape[2] - 1, step=1, value=slice_idx, description=\"Slice:\"),\n",
    "    )\n",
    "    return prediction, decoded_combined, exact_match\n",
    "\n",
    "def list_test_subjects(n=20):\n",
    "    print(f\"Test subjects: {len(inference_dataset.subject_ids)}\")\n",
    "    for i, sid in enumerate(inference_dataset.subject_ids[:n]):\n",
    "        print(f\"{i}: {sid}\")\n",
    "\n",
    "list_test_subjects(5)\n",
    "# verify_encoding_by_id(inference_dataset.subject_ids[2])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 15542776,
     "sourceId": 129601,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 576416,
     "modelInstanceId": 563906,
     "sourceId": 739319,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
